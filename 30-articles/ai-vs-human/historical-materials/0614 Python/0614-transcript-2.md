I'm coming up on the second newsletter. I've got another week and a half. It's Saturday. I thought I would take another stab at the tracker for AI/human inputs on the articles for transparency, ethics, etc. I've been working with Claude 4, which is a new model since I tried Claude last, and it has been going incredibly well—shockingly so considering all the trouble I've had in the past few weeks, if not month, trying to get this achieved. It's been like five different attempts. Some got super close, some used questionable measures, but I've learned a lot along the way.

I had no idea CoLab even existed at that point in time. I didn't know enough to have it break it apart. I simply didn't think about it. I know you need to break complex things up, but it was doing Python, and I don't know anything about Python. I specified the environment, I specified I wanted it in steps. I gave it fairly granular instructions, which are all new. Doing it in CoLab is new. I've been trying to execute it by Python. I could cut and paste into chat windows. One of them was executable in GitHub. So whenever I uploaded something, it would check the folders, or it was supposed to check the folders and then provide what I needed at the appropriate times. When all the files were present, that bombed horribly—that one was a really bad failure. I have notes on all of those, so I need to find that because that'll help me demonstrate the pain I've been through to get here. It has been painful. There are probably parts of it I can't even talk through in an article because there are just long strings of expletives that wouldn't make sense without them. I'm not going to publish that. I'm going to try and keep my sailor mouth to myself.

We've had three real errors. But this is doing so much more than I actually got before. It's loading the libraries with Python. It's mounting my drive. It loads several classes, or functions, I think is how I would think about it. Each of these are larger than any of the Python scripts I got that were supposed to do the whole thing, and that's just to do the counting on the content. There's that much rigor going into actually understanding what a sentence is, actually understanding what a word is. There's like 15 lines of regex in here someplace, which was a big problem for one of them because it couldn't quite get the regex right and kept stumbling on that.

We have a second phase where it loads more classes. Each of these functions is like twice as big as anything I've had before. Oh, it finished. We've done the counts, we've done the attribution mapping—I don't even know what that is. Origin distribution: I've got edited from origin, 85.7%. Draft one sentence, 2.4%. That doesn't seem right, because I changed 36 sentences. Oh, it's saying that only one sentence from the draft survived to the final. I can see that. I practically rewrote this one. Modification, high similarity, medium similarity, low similarity, new content.

It saved the files. I want to look at those. It looks like markup languages for JSON. I might need JSON there, but I can worry about that later.

I think it's been maybe two hours, two and a half hours, and I might actually be at the point of being complete and having it actually do it correctly. I don't think it's complete. I think it has measured the counts and given me percentages off that, but I don't think it's actually gotten to semantics and lexile changes, which is what I really want. Who cares what word I or the AI put in? It's the concept, the finished intent and outcome. What do I want to understand? What do I want to share? I don't care about character counts.

I'll check back in on that later. First, I'm going to paste the results. It's looking through files, generated key insights, next steps. The foundation is now complete and working perfectly. It's telling me to go and look at the things. Ready to build the visualization. I love this. It was so much trouble to get this far.

It's still doubling up on the folders. It's a minor thing. I can probably hunt that down and fix it myself. JSON things open in Sublime anyway. Market, complain, use, work, progression change percent: 3.4 edited. Makes sense. Modification looks good. Markup language is complete.

I'll come back to you. Markup languages, checkpoint. This is the data from the first chunk. This is the markup languages footer. This is just a stick in the footer. That's nice. Then this is everything, and this is the archive I specified. It doesn't need to be human-friendly; it needs to be AI-friendly so I can use the numbers again in the future when I write the long-term thing. It's given me 14 decimals on almost everything.

This is a lot of data. I think it actually has every single sentence in here. I can literally read through these and see if it screwed up on the sentences. There are some mistakes. I'm seeing them already. Your diagram or approach, structured verse—it counted a period there, but did not use the rest as a second sentence, so the sentence was counted, but the character count is off. Again, I don't care about that. It has lexical and semantic statistics here too.

It is every single sentence, or at least it's trying to.

I'm feeling a little stupefied at the moment. It's worth it. I probably lost a cumulative two days on the first several and had mediocre results. I just didn't even bother using them in the first few articles, but this time—well, I guess we're going to do the visualizations now, so chat soon.

Ta.