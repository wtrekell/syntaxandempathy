# The Multi-AI Framework Ideation Experiment: A Complete Documentation

*Testing how different AI models apply creative frameworks under competitive conditions*

## Executive Summary

After spending three years watching the UX design community cycle through AI hype without systematic evaluation, I designed a controlled experiment to test how different AI models handle creative ideation frameworks. The question: Do AI models produce meaningfully different results when using the same creative thinking tools that human design teams rely on?

**Key Finding**: AI models demonstrate distinct "personalities" when applying frameworks like SCAMPER, Six Thinking Hats, and Lotus Blossom, with Claude 4 Sonnet showing methodical depth, ChatGPT 4o displaying confident synthesis, and Gemini demonstrating collaborative nuance. More surprisingly, the competitive evaluation structure—where each AI judged others' work—revealed significant inconsistencies in how these models assess quality and relevance.

The experiment produced clear evidence that framework choice matters less than prompt structure and evaluation methodology when working with AI creative partners. Professional tone consistently outperformed other approaches across all models, while excited tone failed universally—a pattern that contradicts popular assumptions about AI enthusiasm.

## Hypothesis & Setup

### The Central Question

Most UX teams are experimenting with AI tools for ideation, but without systematic comparison of how different models handle the same creative challenges. My hypothesis: Different AI models would demonstrate measurably distinct approaches to creative frameworks, and this difference would be consistent enough to guide tool selection for specific types of design problems.

### Why This Test Mattered

Three decades of working with emerging technologies has taught me that early adoption requires evidence, not enthusiasm. The design community needed data about how AI models actually perform creative tasks, not marketing claims or anecdotal experiences.

The choice to test ideation frameworks specifically came from observing how these tools shape human creative processes. If AI models were going to become creative partners rather than just content generators, understanding their framework compatibility would determine their utility.

### Experimental Context

This experiment builds on my broader investigation into AI-assisted design workflows. Previous tests had shown significant variation in AI performance depending on prompt structure and task complexity, but hadn't isolated the framework variable or tested competitive evaluation scenarios.

The UX design focus was deliberate—this field combines strategic thinking, user empathy, and practical constraints in ways that stress-test AI capabilities beyond simple content generation.

## Methodology Section

### Test Architecture

**Phase 1: Parallel Framework Application**
- Three AI models (Claude 4 Sonnet, ChatGPT 4o, Gemini 2.5 Pro)
- Three creative frameworks (SCAMPER, Six Thinking Hats, Lotus Blossom)
- Identical prompt structure across all combinations
- 15 article concepts generated per model (5 per framework)

**Phase 2: Cross-Evaluation**
- Each AI evaluated the other two models' outputs
- Standardized scoring rubric: Trend Fit (0-10), Novelty (0-10), Practical Impact (0-10)
- Addition of 5 new concepts by each evaluator
- Fresh re-scoring of all 15 concepts (10 external + 5 own)

**Phase 3: Final Adjudication**
- Each AI selected top 2 concepts from pool of 6 finalists
- Final winner selection with detailed justification
- Analysis of consistency in evaluation criteria

### Prompt Engineering Strategy

The foundational prompt structure included:

1. **Research Phase**: Current UX/design trend analysis
2. **Ideation Phase**: Framework-specific concept generation
3. **Evaluation Phase**: Systematic scoring and ranking
4. **Documentation Phase**: Structured output with consistent formatting

**Critical Design Decision**: I used XML structuring throughout to ensure consistent parsing across different AI architectures. This choice proved essential when comparing outputs—each model handled the structured requirements differently despite identical inputs.

### Framework Implementation Details

**SCAMPER Application**:
- Substitute: Replace traditional design approaches
- Combine: Merge separate design disciplines
- Adapt: Modify existing patterns for new contexts
- Modify: Scale or adjust current practices
- Put to other uses: Apply patterns to new domains
- Eliminate: Remove problematic elements
- Reverse: Invert traditional approaches

**Six Thinking Hats Application**:
- White Hat: Factual, data-driven analysis
- Red Hat: Emotional, intuitive responses
- Black Hat: Critical, risk-focused evaluation
- Yellow Hat: Optimistic, benefit-focused thinking
- Green Hat: Creative, alternative generation
- Blue Hat: Process management and overview

**Lotus Blossom Application**:
- Central theme: Core design challenge
- Eight petals: Related concepts branching from center
- Secondary blooms: Each petal becomes new center
- Systematic expansion: Comprehensive topic exploration

### Evaluation Methodology

Each AI used identical scoring criteria but demonstrated different interpretation patterns:

**Trend Fit (0-10)**: Relevance to current UX/product/service design trends
- Claude prioritized regulatory compliance and ethical considerations
- ChatGPT emphasized market adoption and business impact
- Gemini balanced innovation with practical implementation

**Novelty (0-10)**: Freshness versus existing literature
- Claude favored methodological rigor and systematic approaches
- ChatGPT rewarded bold predictions and paradigm shifts
- Gemini valued unique combinations of existing concepts

**Practical Impact (0-10)**: Immediate usefulness to working designers
- Claude focused on detailed implementation guidance
- ChatGPT emphasized strategic business value
- Gemini prioritized collaborative and team-oriented solutions

### Quality Control Measures

**Consistency Checks**:
- Identical prompt timing (all within 2-hour window)
- Version control for all prompt variations
- Raw output preservation before any human editing
- Systematic documentation of model-specific responses

**Bias Detection**:
- Framework rotation to prevent order effects
- Cross-validation through competitive evaluation
- Human oversight on scoring interpretation
- Documentation of unexpected patterns or outliers

## Results & Data

### Framework Performance Patterns

**SCAMPER Results**:
All models demonstrated clear competency with SCAMPER's systematic structure. Claude produced the most methodologically rigorous applications, often including detailed implementation steps. ChatGPT generated the most ambitious substitutions and combinations. Gemini showed particular strength in the "Adapt" and "Modify" categories, suggesting nuanced understanding of incremental change.

Unexpected finding: The "Eliminate" category consistently produced the most practically applicable concepts across all models, suggesting AI tools excel at identifying inefficiencies more than generating novel additions.

**Six Thinking Hats Results**:
This framework revealed the most significant personality differences between models. Claude's "Black Hat" thinking demonstrated sophisticated risk analysis that included regulatory, ethical, and technical considerations. ChatGPT's "Yellow Hat" optimism was data-driven and business-focused. Gemini's "Green Hat" creativity consistently combined multiple perspectives in novel ways.

**Lotus Blossom Results**:
The most challenging framework for all models, with frequent confusion about the expansion methodology. Claude approached it most systematically, treating each petal as a mini-research project. ChatGPT used it more intuitively, creating thematic connections rather than systematic expansions. Gemini produced the most visually coherent concept maps.

### Cross-Evaluation Scoring Analysis

**Scoring Consistency Issues**:
When evaluating each other's work, the models showed concerning variability in applying the rubric:

- **Claude as evaluator**: Consistently harsh on novelty scores, generous with practical impact
- **ChatGPT as evaluator**: Optimistic across all categories, with compression toward middle scores
- **Gemini as evaluator**: Most consistent with initial self-scoring patterns

**Bias Detection**:
Each model showed preference patterns when evaluating concepts:
- Claude favored methodological rigor and ethical considerations
- ChatGPT preferred business impact and strategic thinking
- Gemini valued collaborative approaches and practical accessibility

### Final Competition Results

**Round 1 Winners by Model**:
- Claude: "AI‑Driven Empathy Mapping" (SCAMPER-Substitute, Score: 27)
- ChatGPT: "The Green Hat: Beyond Brainstorms" (Six Thinking Hats-Green Hat, Score: 28)  
- Gemini: "The Conductor: Orchestrating Human & AI Design Teams" (Six Thinking Hats-Blue Hat, Score: 29)

**Final Adjudication Result**:
After evaluating all six finalists, the ultimate winner was "The Post-App Era: Substituting Apps with AI Agents" (SCAMPER-Substitute, Score: 27), selected for its combination of strategic timing, business impact, and fresh perspective on a transformational technology shift.

### Anomaly Documentation

**Unexpected Patterns**:

1. **Framework Complexity Inverse Relationship**: Simpler frameworks (SCAMPER) produced more immediately actionable results than complex frameworks (Lotus Blossom)

2. **Evaluation Drift**: Each model's scoring became more generous when evaluating external work, suggesting calibration challenges

3. **Tone Sensitivity**: Professional tone consistently outperformed other approaches, while excited tone failed universally—contradicting assumptions about AI enthusiasm

4. **Synthesis Capability**: When forced to choose between concepts, all models demonstrated sophisticated synthesis thinking, combining elements from multiple concepts rather than selecting existing options

## Technical Deep Dive

### Prompt Engineering Evolution

The experiment required four distinct prompt types, each optimized for different AI capabilities:

**Type 1: Research + Ideation Prompt**
```xml
<intro>You are a research assistant for UX, product, and service designers.</intro>
<taskOverview>
  <step order="1">Quick scan: Search the web for current trends</step>
  <step order="2">Ideation: Brainstorm 15 concepts using three frameworks</step>
  <!-- Additional structure -->
</taskOverview>
```

**Type 2: Cross-Evaluation Prompt**
```xml
<intro>You are a follow-up assessor for article concepts you did not author.</intro>
<taskOverview>
  <step order="1">Score external ideas using established rubric</step>
  <step order="2">Create 5 new concepts</step>
  <!-- Evaluation methodology -->
</taskOverview>
```

**Type 3: Final Adjudication Prompt**
```xml
<intro>You are the final adjudicator for six champion concepts.</intro>
<taskOverview>
  <step order="1">Fresh evaluation of all finalists</step>
  <step order="2">Select single best concept with justification</step>
</taskOverview>
```

### Failure Modes Identified

**Framework Misapplication**:
- Lotus Blossom confusion: Models frequently treated petals as independent concepts rather than systematic expansions
- Six Thinking Hats contamination: Bleeding between different hat perspectives, especially Green and Yellow
- SCAMPER literalism: Over-rigid interpretation of verbs without creative synthesis

**Evaluation Inconsistencies**:
- Score inflation during cross-evaluation phases
- Rubric drift: Different interpretation of scoring criteria between models
- Recency bias: Later concepts scored higher regardless of objective quality

**Output Format Struggles**:
- Markdown table formatting inconsistencies
- JSON structure variations despite explicit templates
- Citation and reference handling differences

### Edge Cases and Unexpected Behaviors

**Claude Specifics**:
- Exceptional at regulatory and compliance considerations
- Conservative scoring that required recalibration
- Detailed implementation focus sometimes at expense of big-picture thinking

**ChatGPT Specifics**:
- Confident synthesis across multiple concepts
- Business-oriented framing of all concepts
- Optimistic scoring that compressed range between good and excellent

**Gemini Specifics**:
- Strong collaborative and team-oriented concept generation
- Most consistent self-evaluation patterns
- Nuanced understanding of practical implementation challenges

### Technical Implementation Notes

**API Considerations**:
- Rate limiting affected cross-evaluation timing
- Context window management required prompt optimization
- Temperature settings influenced creativity vs. consistency trade-offs

**Data Management**:
- Complete conversation logs preserved for analysis
- Scoring matrices exported for statistical analysis
- Prompt variations documented for replication

## Implications

### For AI-Assisted Design Practice

**Framework Selection Guidance**:
Based on these results, design teams should consider:
- SCAMPER for tactical problem-solving and process improvement
- Six Thinking Hats for strategic planning and comprehensive analysis
- Lotus Blossom for systematic exploration of complex problem spaces

**Model Selection Strategy**:
- Claude 4 Sonnet: Best for methodical analysis and ethical considerations
- ChatGPT 4o: Optimal for business strategy and synthesis thinking
- Gemini 2.5 Pro: Ideal for collaborative processes and practical implementation

### For Prompt Engineering Methodology

**Critical Success Factors**:
1. **Structured Templates**: XML formatting significantly improved consistency across models
2. **Explicit Rubrics**: Detailed scoring criteria essential, but interpretation varies
3. **Competitive Dynamics**: Cross-evaluation reveals model biases and scoring patterns
4. **Professional Tone**: Consistently outperforms informal or excited approaches

### For Creative Framework Application

**Evidence-Based Recommendations**:
- Framework complexity should match task complexity—simpler tools often produce better immediate results
- Competitive evaluation structures create more rigorous output but require calibration
- AI models demonstrate framework preferences that align with their underlying training patterns

### Broader Design Community Impact

This experiment demonstrates that AI creative partnership requires the same systematic approach as any other design tool evaluation. The "just try it and see" approach popular in early AI adoption misses crucial performance patterns that affect practical utility.

The evidence suggests AI models are ready for systematic creative collaboration, but teams need framework-specific strategies and evaluation methodologies to maximize effectiveness.

### Future Research Directions

**Immediate Next Steps**:
1. Framework-specific prompt optimization based on model strengths
2. Longitudinal study of scoring consistency across multiple evaluation sessions
3. Human designer evaluation of AI-generated concepts for validation

**Broader Investigation Opportunities**:
1. Cross-cultural evaluation of AI creative frameworks
2. Team-based AI integration with multiple models
3. Real-world implementation testing of winning concepts

The systematic documentation of these patterns creates a foundation for evidence-based AI creative partnership—moving beyond hype toward practical methodology that enhances rather than replaces human creative capabilities.