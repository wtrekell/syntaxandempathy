{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+XVcKo661xPPttBpKkEzZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wtrekell/syntaxandempathy/blob/main/ai_vs_human.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_8AIVCB-cME",
        "outputId": "c411c111-02d5-4fc9-b1c2-dc6c58732ae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úì Drive mounted successfully\n",
            "üìã Configuration loaded. Ready to process articles.\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 1: SETUP AND CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from google.colab import drive, files\n",
        "import zipfile\n",
        "\n",
        "# Mount Google Drive (will prompt for authorization)\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úì Drive mounted successfully\")\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    \"\"\"Configuration settings for the analysis\"\"\"\n",
        "\n",
        "    # File naming conventions\n",
        "    VERSION_PREFIXES = ['draft-', 'refined-', 'edited-', 'final-']\n",
        "    VERSION_ORDER = {prefix: i for i, prefix in enumerate(VERSION_PREFIXES)}\n",
        "\n",
        "    # Analysis settings\n",
        "    MIN_SENTENCE_LENGTH = 10  # Minimum characters for a sentence\n",
        "    MAX_SENTENCE_LENGTH = 1000  # Maximum characters for a sentence\n",
        "\n",
        "def setup_output_directories(base_path):\n",
        "    \"\"\"Create necessary output directories\"\"\"\n",
        "    # Don't create nested folders - use the base path directly\n",
        "    output_dir = base_path\n",
        "    archive_dir = os.path.join(base_path, 'archive')\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(archive_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"‚úì Output directory ready: {output_dir}\")\n",
        "    print(f\"‚úì Archive directory ready: {archive_dir}\")\n",
        "\n",
        "    return output_dir, archive_dir\n",
        "\n",
        "print(\"üìã Configuration loaded. Ready to process articles.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 2: DATA INGESTION & VALIDATION (STEP 1)\n",
        "# =============================================================================\n",
        "\n",
        "class ArticleVersions:\n",
        "    \"\"\"Class to handle loading and validating article versions\"\"\"\n",
        "\n",
        "    def __init__(self, article_name, input_path):\n",
        "        self.article_name = article_name\n",
        "        self.input_path = input_path\n",
        "        self.versions = {}\n",
        "        self.metadata = {\n",
        "            'article_name': article_name,\n",
        "            'input_path': input_path,\n",
        "            'processing_timestamp': datetime.now().isoformat(),\n",
        "            'versions_found': [],\n",
        "            'validation_status': 'pending'\n",
        "        }\n",
        "\n",
        "    def load_versions(self):\n",
        "        \"\"\"Load all versions of an article from the specified path\"\"\"\n",
        "        print(f\"\\nüìÅ Loading versions for article: {self.article_name}\")\n",
        "        print(f\"üìÇ Input path: {self.input_path}\")\n",
        "\n",
        "        # Find all files matching the article name pattern\n",
        "        for prefix in Config.VERSION_PREFIXES:\n",
        "            filename = f\"{prefix}{self.article_name}.md\"\n",
        "            filepath = os.path.join(self.input_path, filename)\n",
        "\n",
        "            if os.path.exists(filepath):\n",
        "                try:\n",
        "                    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "                        content = file.read()\n",
        "                        self.versions[prefix.rstrip('-')] = {\n",
        "                            'filename': filename,\n",
        "                            'filepath': filepath,\n",
        "                            'content': content,\n",
        "                            'loaded_at': datetime.now().isoformat(),\n",
        "                            'file_size': len(content)\n",
        "                        }\n",
        "                        print(f\"  ‚úì Loaded: {filename} ({len(content)} characters)\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚úó Error loading {filename}: {str(e)}\")\n",
        "            else:\n",
        "                print(f\"  - Not found: {filename}\")\n",
        "\n",
        "        self.metadata['versions_found'] = list(self.versions.keys())\n",
        "        return self.versions\n",
        "\n",
        "    def validate_version_sequence(self):\n",
        "        \"\"\"Validate that we have the minimum required versions\"\"\"\n",
        "        found_versions = set(self.versions.keys())\n",
        "        required_versions = ['draft', 'final']\n",
        "\n",
        "        # Check for required versions\n",
        "        missing_required = []\n",
        "        for version in required_versions:\n",
        "            if version not in found_versions:\n",
        "                missing_required.append(version)\n",
        "\n",
        "        # Validation results\n",
        "        validation_results = {\n",
        "            'has_draft': 'draft' in found_versions,\n",
        "            'has_final': 'final' in found_versions,\n",
        "            'missing_required': missing_required,\n",
        "            'versions_found': list(found_versions),\n",
        "            'is_valid': len(missing_required) == 0\n",
        "        }\n",
        "\n",
        "        # Update metadata\n",
        "        self.metadata['validation_results'] = validation_results\n",
        "\n",
        "        if validation_results['is_valid']:\n",
        "            self.metadata['validation_status'] = 'passed'\n",
        "            print(f\"‚úì Validation passed: Found {len(found_versions)} versions\")\n",
        "        else:\n",
        "            self.metadata['validation_status'] = 'failed'\n",
        "            print(f\"‚úó Validation failed: Missing required versions\")\n",
        "            print(f\"  Missing: {', '.join(missing_required)}\")\n",
        "\n",
        "        return validation_results\n",
        "\n",
        "    def get_summary(self):\n",
        "        \"\"\"Get a summary of loaded versions\"\"\"\n",
        "        summary = {\n",
        "            'article_name': self.article_name,\n",
        "            'input_path': self.input_path,\n",
        "            'versions_count': len(self.versions),\n",
        "            'validation_status': self.metadata['validation_status'],\n",
        "            'file_sizes': {}\n",
        "        }\n",
        "\n",
        "        for version, data in self.versions.items():\n",
        "            summary['file_sizes'][version] = data['file_size']\n",
        "\n",
        "        return summary\n",
        "\n",
        "print(\"üìñ ArticleVersions class loaded. Ready for data ingestion.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3ky_mO3Fr3D",
        "outputId": "1dcfe5ec-24c3-4cac-9f32-57b260bc52e2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìñ ArticleVersions class loaded. Ready for data ingestion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 3: TEXT PREPROCESSING (STEP 2)\n",
        "# =============================================================================\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"Class to handle text preprocessing and segmentation\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.processed_versions = {}\n",
        "\n",
        "    def clean_markdown(self, text):\n",
        "        \"\"\"Clean markdown formatting while preserving content structure\"\"\"\n",
        "        # Remove markdown formatting but keep the text\n",
        "        patterns = [\n",
        "            (r'^\\s*#{1,6}\\s+', ''),  # Headers\n",
        "            (r'\\*\\*(.*?)\\*\\*', r'\\1'),  # Bold\n",
        "            (r'\\*(.*?)\\*', r'\\1'),  # Italic\n",
        "            (r'`(.*?)`', r'\\1'),  # Inline code\n",
        "            (r'```.*?```', ''),  # Code blocks\n",
        "            (r'!\\[.*?\\]\\(.*?\\)', ''),  # Images\n",
        "            (r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1'),  # Links\n",
        "            (r'^\\s*[\\*\\-\\+]\\s+', ''),  # Bullet points\n",
        "            (r'^\\s*\\d+\\.\\s+', ''),  # Numbered lists\n",
        "            (r'\\n{3,}', '\\n\\n'),  # Multiple newlines\n",
        "        ]\n",
        "\n",
        "        cleaned_text = text\n",
        "\n",
        "        # Apply patterns that need multiline flag\n",
        "        multiline_patterns = [\n",
        "            (r'^\\s*[\\*\\-\\+]\\s+', ''),  # Bullet points\n",
        "            (r'^\\s*\\d+\\.\\s+', ''),  # Numbered lists\n",
        "        ]\n",
        "\n",
        "        for pattern, replacement in multiline_patterns:\n",
        "            cleaned_text = re.sub(pattern, replacement, cleaned_text, flags=re.MULTILINE)\n",
        "\n",
        "        # Apply regular patterns\n",
        "        regular_patterns = [\n",
        "            (r'^\\s*#{1,6}\\s+', ''),  # Headers\n",
        "            (r'\\*\\*(.*?)\\*\\*', r'\\1'),  # Bold\n",
        "            (r'\\*(.*?)\\*', r'\\1'),  # Italic\n",
        "            (r'`(.*?)`', r'\\1'),  # Inline code\n",
        "            (r'```.*?```', ''),  # Code blocks\n",
        "            (r'!\\[.*?\\]\\(.*?\\)', ''),  # Images\n",
        "            (r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1'),  # Links\n",
        "            (r'\\n{3,}', '\\n\\n'),  # Multiple newlines\n",
        "        ]\n",
        "\n",
        "        for pattern, replacement in regular_patterns:\n",
        "            cleaned_text = re.sub(pattern, replacement, cleaned_text)\n",
        "\n",
        "        return cleaned_text.strip()\n",
        "\n",
        "    def segment_into_sentences(self, text):\n",
        "        \"\"\"Segment text into sentences with basic filtering\"\"\"\n",
        "        # Simple sentence segmentation (can be enhanced with spaCy later if needed)\n",
        "        sentences = re.split(r'[.!?]+\\s+', text)\n",
        "\n",
        "        # Filter sentences\n",
        "        filtered_sentences = []\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if (Config.MIN_SENTENCE_LENGTH <= len(sentence) <= Config.MAX_SENTENCE_LENGTH\n",
        "                and sentence):\n",
        "                filtered_sentences.append(sentence)\n",
        "\n",
        "        return filtered_sentences\n",
        "\n",
        "    def segment_into_paragraphs(self, text):\n",
        "        \"\"\"Segment text into paragraphs\"\"\"\n",
        "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "        return paragraphs\n",
        "\n",
        "    def process_version(self, version_name, raw_content):\n",
        "        \"\"\"Process a single version of the article\"\"\"\n",
        "        print(f\"  Processing {version_name} version...\")\n",
        "\n",
        "        # Clean the markdown\n",
        "        cleaned_content = self.clean_markdown(raw_content)\n",
        "\n",
        "        # Segment into different units\n",
        "        sentences = self.segment_into_sentences(cleaned_content)\n",
        "        paragraphs = self.segment_into_paragraphs(cleaned_content)\n",
        "\n",
        "        # Calculate basic statistics\n",
        "        stats = {\n",
        "            'character_count': len(cleaned_content),\n",
        "            'word_count': len(cleaned_content.split()),\n",
        "            'sentence_count': len(sentences),\n",
        "            'paragraph_count': len(paragraphs),\n",
        "            'avg_sentence_length': sum(len(s) for s in sentences) / len(sentences) if sentences else 0,\n",
        "            'avg_paragraph_length': sum(len(p) for p in paragraphs) / len(paragraphs) if paragraphs else 0\n",
        "        }\n",
        "\n",
        "        processed_data = {\n",
        "            'version_name': version_name,\n",
        "            'raw_content': raw_content,\n",
        "            'cleaned_content': cleaned_content,\n",
        "            'sentences': sentences,\n",
        "            'paragraphs': paragraphs,\n",
        "            'statistics': stats,\n",
        "            'processed_at': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        self.processed_versions[version_name] = processed_data\n",
        "\n",
        "        print(f\"    ‚úì {stats['sentence_count']} sentences, {stats['paragraph_count']} paragraphs\")\n",
        "        print(f\"    ‚úì {stats['word_count']} words, {stats['character_count']} characters\")\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def process_all_versions(self, article_versions):\n",
        "        \"\"\"Process all versions of an article\"\"\"\n",
        "        print(f\"\\nüîÑ Preprocessing text for all versions...\")\n",
        "\n",
        "        for version_name, version_data in article_versions.versions.items():\n",
        "            self.process_version(version_name, version_data['content'])\n",
        "\n",
        "        return self.processed_versions\n",
        "\n",
        "    def get_processing_summary(self):\n",
        "        \"\"\"Get a summary of processing results\"\"\"\n",
        "        summary = {}\n",
        "        for version_name, data in self.processed_versions.items():\n",
        "            summary[version_name] = data['statistics']\n",
        "\n",
        "        return summary\n",
        "\n",
        "print(\"üîß TextPreprocessor class loaded. Ready for text processing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhHY2pJqFvOr",
        "outputId": "255285df-9145-4017-9905-201bf4c65f89"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß TextPreprocessor class loaded. Ready for text processing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 4: EXECUTION FUNCTIONS AND CHECKPOINT MANAGEMENT\n",
        "# =============================================================================\n",
        "\n",
        "def save_checkpoint_data(article_versions, preprocessor, output_path, checkpoint_name=\"steps_1_2\"):\n",
        "    \"\"\"Save checkpoint data for review\"\"\"\n",
        "    checkpoint_data = {\n",
        "        'checkpoint_name': checkpoint_name,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'article_metadata': article_versions.metadata,\n",
        "        'processing_summary': preprocessor.get_processing_summary(),\n",
        "        'validation_results': article_versions.metadata.get('validation_results', {}),\n",
        "        'article_summary': article_versions.get_summary()\n",
        "    }\n",
        "\n",
        "    # Save to output directory\n",
        "    checkpoint_file = f\"{output_path}/{article_versions.article_name}_checkpoint_{checkpoint_name}.json\"\n",
        "\n",
        "    with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\nüíæ Checkpoint saved: {checkpoint_file}\")\n",
        "    return checkpoint_data\n",
        "\n",
        "def run_steps_1_2(article_name, input_path, base_output_path):\n",
        "    \"\"\"Run steps 1-2 for a given article\"\"\"\n",
        "    print(f\"üöÄ Starting Steps 1-2 for article: {article_name}\")\n",
        "    print(f\"üìÇ Input path: {input_path}\")\n",
        "\n",
        "    # Setup output directories\n",
        "    output_dir, archive_dir = setup_output_directories(base_output_path)\n",
        "\n",
        "    # Step 1: Data Ingestion & Validation\n",
        "    article_versions = ArticleVersions(article_name, input_path)\n",
        "    article_versions.load_versions()\n",
        "    validation_results = article_versions.validate_version_sequence()\n",
        "\n",
        "    if not validation_results['is_valid']:\n",
        "        print(\"‚ùå Cannot proceed: Missing required versions (draft and final)\")\n",
        "        return None, None\n",
        "\n",
        "    # Step 2: Text Preprocessing\n",
        "    preprocessor = TextPreprocessor()\n",
        "    preprocessor.process_all_versions(article_versions)\n",
        "\n",
        "    # Save checkpoint\n",
        "    checkpoint_data = save_checkpoint_data(article_versions, preprocessor, output_dir)\n",
        "\n",
        "    print(f\"\\n‚úÖ Steps 1-2 completed successfully!\")\n",
        "    print(f\"üìä Processing Summary:\")\n",
        "    for version, stats in preprocessor.get_processing_summary().items():\n",
        "        print(f\"  {version}: {stats['word_count']} words, {stats['sentence_count']} sentences\")\n",
        "\n",
        "    return article_versions, preprocessor\n",
        "\n",
        "# Interactive input functions\n",
        "def get_user_inputs():\n",
        "    \"\"\"Get user inputs for processing\"\"\"\n",
        "    print(\"üìù Please provide the following information:\")\n",
        "\n",
        "    article_name = input(\"Enter article name (without .md extension): \").strip()\n",
        "    input_path = input(\"Enter full path to input folder containing markdown files: \").strip()\n",
        "    base_output_path = input(\"Enter full path to base output folder: \").strip()\n",
        "\n",
        "    print(f\"\\nüìã Configuration:\")\n",
        "    print(f\"  Article name: {article_name}\")\n",
        "    print(f\"  Input path: {input_path}\")\n",
        "    print(f\"  Output path: {base_output_path}\")\n",
        "\n",
        "    confirm = input(\"\\nProceed with these settings? (y/n): \").strip().lower()\n",
        "\n",
        "    if confirm == 'y':\n",
        "        return article_name, input_path, base_output_path\n",
        "    else:\n",
        "        print(\"‚ùå Cancelled. Run get_user_inputs() again to restart.\")\n",
        "        return None, None, None\n",
        "\n",
        "def process_article_interactive():\n",
        "    \"\"\"Process an article with interactive inputs\"\"\"\n",
        "    article_name, input_path, base_output_path = get_user_inputs()\n",
        "\n",
        "    if article_name and input_path and base_output_path:\n",
        "        return run_steps_1_2(article_name, input_path, base_output_path)\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "print(\"üìã Ready to process your article!\")\n",
        "print(\"Run: article_versions, preprocessor = process_article_interactive()\")\n",
        "print(\"\\nMake sure your markdown files are named:\")\n",
        "print(\"- draft-your-article-name.md\")\n",
        "print(\"- refined-your-article-name.md\")\n",
        "print(\"- edited-your-article-name.md\")\n",
        "print(\"- final-your-article-name.md\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4bdYVJdFyQD",
        "outputId": "fb4dc86c-5e4b-40c1-f921-faa320b7447b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìã Ready to process your article!\n",
            "Run: article_versions, preprocessor = process_article_interactive()\n",
            "\n",
            "Make sure your markdown files are named:\n",
            "- draft-your-article-name.md\n",
            "- refined-your-article-name.md\n",
            "- edited-your-article-name.md\n",
            "- final-your-article-name.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 5: SAMPLE DATA CREATOR (FOR TESTING ONLY)\n",
        "# =============================================================================\n",
        "\n",
        "def create_sample_files_for_testing(output_path):\n",
        "    \"\"\"Create sample markdown files for testing (run this once to test)\"\"\"\n",
        "    sample_content = {\n",
        "        'draft-': \"\"\"# Sample Article\n",
        "\n",
        "This is a draft article about artificial intelligence and its impact on society. AI has revolutionized many industries.\n",
        "\n",
        "The technology continues to evolve rapidly. Machine learning algorithms are becoming more sophisticated every day.\n",
        "\n",
        "We must consider the ethical implications of AI development.\"\"\",\n",
        "\n",
        "        'refined-': \"\"\"# Sample Article\n",
        "\n",
        "This is a refined article examining artificial intelligence and its transformative impact on modern society. AI has fundamentally revolutionized numerous industries across the globe.\n",
        "\n",
        "The technology continues to evolve at an unprecedented pace. Advanced machine learning algorithms are becoming increasingly sophisticated with each passing day.\n",
        "\n",
        "We must carefully consider the complex ethical implications of AI development and deployment.\"\"\",\n",
        "\n",
        "        'edited-': \"\"\"# Sample Article\n",
        "\n",
        "This comprehensive article examines artificial intelligence and its transformative impact on modern society. AI has fundamentally revolutionized numerous industries worldwide, reshaping how we work and live.\n",
        "\n",
        "The technology continues to evolve at an unprecedented pace, driven by breakthrough innovations. Advanced machine learning algorithms are becoming increasingly sophisticated, enabling new applications we never thought possible.\n",
        "\n",
        "We must carefully consider the complex ethical implications of AI development and deployment, ensuring responsible innovation for the benefit of humanity.\"\"\",\n",
        "\n",
        "        'final-': \"\"\"# Sample Article\n",
        "\n",
        "This comprehensive article examines artificial intelligence and its transformative impact on modern society. AI has fundamentally revolutionized numerous industries worldwide, reshaping how we work, communicate, and live.\n",
        "\n",
        "The technology continues to evolve at an unprecedented pace, driven by breakthrough innovations in computing power and algorithmic design. Advanced machine learning algorithms are becoming increasingly sophisticated, enabling new applications we never thought possible just a decade ago.\n",
        "\n",
        "We must carefully consider the complex ethical implications of AI development and deployment, ensuring responsible innovation that serves the benefit of all humanity while mitigating potential risks.\"\"\"\n",
        "    }\n",
        "\n",
        "    article_name = \"sample-article\"\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    for prefix, content in sample_content.items():\n",
        "        filename = f\"{prefix}{article_name}.md\"\n",
        "        filepath = os.path.join(output_path, filename)\n",
        "\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "\n",
        "        print(f\"Created: {filename}\")\n",
        "\n",
        "    return article_name, output_path\n",
        "\n",
        "print(\"\\nüß™ Sample data creator available for testing if needed.\")\n",
        "print(\"To create test files, run:\")\n",
        "print(\"sample_name, sample_path = create_sample_files_for_testing('/your/test/path')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lipLfDL1F1LD",
        "outputId": "4cf22033-eadf-4c1f-832d-7dc723afbdec"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß™ Sample data creator available for testing if needed.\n",
            "To create test files, run:\n",
            "sample_name, sample_path = create_sample_files_for_testing('/your/test/path')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article_versions, preprocessor = process_article_interactive()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-hysVC7J3F0",
        "outputId": "13918ac7-8998-49f1-d007-bcb97b45a556"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Please provide the following information:\n",
            "Enter article name (without .md extension): markup-languages\n",
            "Enter full path to input folder containing markdown files: /content/drive/MyDrive/Google Drive/syntaxandempathy/30-articles/ai-vs-human\n",
            "Enter full path to base output folder: /content/drive/MyDrive/Google Drive/syntaxandempathy/30-articles/ai-vs-human/output/output\n",
            "\n",
            "üìã Configuration:\n",
            "  Article name: markup-languages\n",
            "  Input path: /content/drive/MyDrive/Google Drive/syntaxandempathy/30-articles/ai-vs-human\n",
            "  Output path: /content/drive/MyDrive/Google Drive/syntaxandempathy/30-articles/ai-vs-human/output/output\n",
            "\n",
            "Proceed with these settings? (y/n): y\n",
            "üöÄ Starting Steps 1-2 for article: markup-languages\n",
            "üìÇ Input path: /content/drive/MyDrive/Google Drive/syntaxandempathy/30-articles/ai-vs-human\n",
            "‚úì Output directory ready: /content/drive/MyDrive/Google Drive/syntaxandempathy/30-articles/ai-vs-human/output/output\n",
            "‚úì Archive directory ready: /content/drive/MyDrive/Google Drive/syntaxandempathy/30-articles/ai-vs-human/output/output/archive\n",
            "\n",
            "üìÅ Loading versions for article: markup-languages\n",
            "üìÇ Input path: /content/drive/MyDrive/Google Drive/syntaxandempathy/30-articles/ai-vs-human\n",
            "  ‚úì Loaded: draft-markup-languages.md (5667 characters)\n",
            "  ‚úì Loaded: refined-markup-languages.md (7301 characters)\n",
            "  ‚úì Loaded: edited-markup-languages.md (7575 characters)\n",
            "  ‚úì Loaded: final-markup-languages.md (6327 characters)\n",
            "‚úì Validation passed: Found 4 versions\n",
            "\n",
            "üîÑ Preprocessing text for all versions...\n",
            "  Processing draft version...\n",
            "    ‚úì 47 sentences, 29 paragraphs\n",
            "    ‚úì 863 words, 5535 characters\n",
            "  Processing refined version...\n",
            "    ‚úì 60 sentences, 31 paragraphs\n",
            "    ‚úì 1128 words, 7213 characters\n",
            "  Processing edited version...\n",
            "    ‚úì 45 sentences, 35 paragraphs\n",
            "    ‚úì 1066 words, 7479 characters\n",
            "  Processing final version...\n",
            "    ‚úì 42 sentences, 38 paragraphs\n",
            "    ‚úì 892 words, 6258 characters\n",
            "\n",
            "üíæ Checkpoint saved: /content/drive/MyDrive/Google Drive/syntaxandempathy/30-articles/ai-vs-human/output/output/markup-languages_checkpoint_steps_1_2.json\n",
            "\n",
            "‚úÖ Steps 1-2 completed successfully!\n",
            "üìä Processing Summary:\n",
            "  draft: 863 words, 47 sentences\n",
            "  refined: 1128 words, 60 sentences\n",
            "  edited: 1066 words, 45 sentences\n",
            "  final: 892 words, 42 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers scikit-learn -q\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import difflib\n",
        "from collections import defaultdict\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"üì¶ Dependencies installed and imported successfully!\")\n",
        "print(\"ü§ñ Loading SentenceTransformer model (this may take a moment)...\")\n",
        "\n",
        "# Load the semantic similarity model\n",
        "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"‚úÖ Semantic model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGzjFLxSMguC",
        "outputId": "aee7b997-9a42-4b82-f5b1-3d9a9e20be08"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Dependencies installed and imported successfully!\n",
            "ü§ñ Loading SentenceTransformer model (this may take a moment)...\n",
            "‚úÖ Semantic model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 2: SIMILARITY ANALYSIS (STEP 3)\n",
        "# =============================================================================\n",
        "\n",
        "class SimilarityAnalyzer:\n",
        "    \"\"\"Class to handle lexical and semantic similarity analysis\"\"\"\n",
        "\n",
        "    def __init__(self, semantic_model):\n",
        "        self.semantic_model = semantic_model\n",
        "        self.similarity_results = {}\n",
        "\n",
        "    def calculate_lexical_similarity(self, text1, text2):\n",
        "        \"\"\"Calculate lexical similarity using multiple metrics\"\"\"\n",
        "        # Jaccard similarity (word-level)\n",
        "        words1 = set(text1.lower().split())\n",
        "        words2 = set(text2.lower().split())\n",
        "        jaccard = len(words1.intersection(words2)) / len(words1.union(words2)) if words1.union(words2) else 0\n",
        "\n",
        "        # Edit distance similarity (character-level)\n",
        "        sequence_matcher = difflib.SequenceMatcher(None, text1, text2)\n",
        "        edit_similarity = sequence_matcher.ratio()\n",
        "\n",
        "        # TF-IDF cosine similarity\n",
        "        vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
        "        try:\n",
        "            tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
        "            tfidf_similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "        except:\n",
        "            tfidf_similarity = 0.0\n",
        "\n",
        "        return {\n",
        "            'jaccard_similarity': jaccard,\n",
        "            'edit_similarity': edit_similarity,\n",
        "            'tfidf_similarity': tfidf_similarity,\n",
        "            'lexical_average': (jaccard + edit_similarity + tfidf_similarity) / 3\n",
        "        }\n",
        "\n",
        "    def calculate_semantic_similarity(self, text1, text2):\n",
        "        \"\"\"Calculate semantic similarity using sentence embeddings\"\"\"\n",
        "        # Get embeddings\n",
        "        embeddings = self.semantic_model.encode([text1, text2])\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "\n",
        "        return {\n",
        "            'semantic_similarity': float(similarity),\n",
        "            'embedding_dim': len(embeddings[0])\n",
        "        }\n",
        "\n",
        "    def calculate_sentence_level_similarities(self, sentences1, sentences2, version1, version2):\n",
        "        \"\"\"Calculate similarities at sentence level between two versions\"\"\"\n",
        "        print(f\"    Analyzing {len(sentences1)} vs {len(sentences2)} sentences...\")\n",
        "\n",
        "        sentence_similarities = []\n",
        "\n",
        "        # Calculate all pairwise similarities\n",
        "        for i, sent1 in enumerate(sentences1):\n",
        "            best_match = {'index': -1, 'lexical': 0, 'semantic': 0, 'combined': 0}\n",
        "\n",
        "            for j, sent2 in enumerate(sentences2):\n",
        "                # Calculate similarities\n",
        "                lexical = self.calculate_lexical_similarity(sent1, sent2)\n",
        "                semantic = self.calculate_semantic_similarity(sent1, sent2)\n",
        "\n",
        "                # Combined score (weighted average)\n",
        "                combined = (lexical['lexical_average'] + semantic['semantic_similarity']) / 2\n",
        "\n",
        "                if combined > best_match['combined']:\n",
        "                    best_match = {\n",
        "                        'index': j,\n",
        "                        'lexical': lexical['lexical_average'],\n",
        "                        'semantic': semantic['semantic_similarity'],\n",
        "                        'combined': combined,\n",
        "                        'target_sentence': sent2\n",
        "                    }\n",
        "\n",
        "            sentence_similarities.append({\n",
        "                'source_index': i,\n",
        "                'source_sentence': sent1,\n",
        "                'best_match': best_match\n",
        "            })\n",
        "\n",
        "        return sentence_similarities\n",
        "\n",
        "    def analyze_version_pair(self, version1_data, version2_data, version1_name, version2_name):\n",
        "        \"\"\"Analyze similarities between two versions\"\"\"\n",
        "        print(f\"  üîç Analyzing {version1_name} ‚Üí {version2_name}\")\n",
        "\n",
        "        # Full text similarity\n",
        "        full_text_lexical = self.calculate_lexical_similarity(\n",
        "            version1_data['cleaned_content'],\n",
        "            version2_data['cleaned_content']\n",
        "        )\n",
        "        full_text_semantic = self.calculate_semantic_similarity(\n",
        "            version1_data['cleaned_content'],\n",
        "            version2_data['cleaned_content']\n",
        "        )\n",
        "\n",
        "        # Sentence-level analysis\n",
        "        sentence_analysis = self.calculate_sentence_level_similarities(\n",
        "            version1_data['sentences'],\n",
        "            version2_data['sentences'],\n",
        "            version1_name,\n",
        "            version2_name\n",
        "        )\n",
        "\n",
        "        # Paragraph-level similarity\n",
        "        para_lexical = self.calculate_lexical_similarity(\n",
        "            ' '.join(version1_data['paragraphs']),\n",
        "            ' '.join(version2_data['paragraphs'])\n",
        "        )\n",
        "        para_semantic = self.calculate_semantic_similarity(\n",
        "            ' '.join(version1_data['paragraphs']),\n",
        "            ' '.join(version2_data['paragraphs'])\n",
        "        )\n",
        "\n",
        "        # Aggregate sentence similarities\n",
        "        sentence_similarities = [s['best_match']['combined'] for s in sentence_analysis]\n",
        "        avg_sentence_similarity = np.mean(sentence_similarities) if sentence_similarities else 0\n",
        "\n",
        "        return {\n",
        "            'version_pair': f\"{version1_name}_to_{version2_name}\",\n",
        "            'full_text': {\n",
        "                'lexical': full_text_lexical,\n",
        "                'semantic': full_text_semantic,\n",
        "                'combined': (full_text_lexical['lexical_average'] + full_text_semantic['semantic_similarity']) / 2\n",
        "            },\n",
        "            'sentence_level': {\n",
        "                'average_similarity': avg_sentence_similarity,\n",
        "                'individual_similarities': sentence_similarities,\n",
        "                'detailed_analysis': sentence_analysis\n",
        "            },\n",
        "            'paragraph_level': {\n",
        "                'lexical': para_lexical,\n",
        "                'semantic': para_semantic,\n",
        "                'combined': (para_lexical['lexical_average'] + para_semantic['semantic_similarity']) / 2\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def analyze_all_versions(self, processed_versions):\n",
        "        \"\"\"Analyze similarities between all version pairs\"\"\"\n",
        "        print(f\"\\nüîç Step 3: Similarity Analysis\")\n",
        "\n",
        "        version_names = list(processed_versions.keys())\n",
        "        version_order = ['draft', 'refined', 'edited', 'final']\n",
        "\n",
        "        # Sort versions by expected order\n",
        "        sorted_versions = []\n",
        "        for expected in version_order:\n",
        "            if expected in version_names:\n",
        "                sorted_versions.append(expected)\n",
        "\n",
        "        # Sequential analysis (draft‚Üírefined‚Üíedited‚Üífinal)\n",
        "        sequential_results = []\n",
        "        for i in range(len(sorted_versions) - 1):\n",
        "            current_version = sorted_versions[i]\n",
        "            next_version = sorted_versions[i + 1]\n",
        "\n",
        "            result = self.analyze_version_pair(\n",
        "                processed_versions[current_version],\n",
        "                processed_versions[next_version],\n",
        "                current_version,\n",
        "                next_version\n",
        "            )\n",
        "            sequential_results.append(result)\n",
        "\n",
        "        # Draft to final comparison\n",
        "        draft_to_final = None\n",
        "        if 'draft' in version_names and 'final' in version_names:\n",
        "            print(f\"  üîç Analyzing draft ‚Üí final (overall change)\")\n",
        "            draft_to_final = self.analyze_version_pair(\n",
        "                processed_versions['draft'],\n",
        "                processed_versions['final'],\n",
        "                'draft',\n",
        "                'final'\n",
        "            )\n",
        "\n",
        "        self.similarity_results = {\n",
        "            'sequential_analysis': sequential_results,\n",
        "            'draft_to_final': draft_to_final,\n",
        "            'analysis_timestamp': datetime.now().isoformat(),\n",
        "            'versions_analyzed': sorted_versions\n",
        "        }\n",
        "\n",
        "        return self.similarity_results\n",
        "\n",
        "print(\"üîç SimilarityAnalyzer class loaded. Ready for similarity analysis.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0OQ62SiMjnr",
        "outputId": "b04ac495-cef3-42f4-abf9-ce370cec3719"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç SimilarityAnalyzer class loaded. Ready for similarity analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 3: ATTRIBUTION MAPPING (STEP 4)\n",
        "# =============================================================================\n",
        "\n",
        "class AttributionMapper:\n",
        "    \"\"\"Class to track content attribution across versions\"\"\"\n",
        "\n",
        "    def __init__(self, similarity_threshold=0.3):\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.attribution_results = {}\n",
        "\n",
        "    def trace_sentence_origins(self, processed_versions, similarity_results):\n",
        "        \"\"\"Trace each final sentence back to its earliest appearance\"\"\"\n",
        "        print(f\"\\nüìç Step 4: Attribution Mapping\")\n",
        "\n",
        "        version_order = ['draft', 'refined', 'edited', 'final']\n",
        "        available_versions = [v for v in version_order if v in processed_versions]\n",
        "\n",
        "        if 'final' not in available_versions:\n",
        "            print(\"‚ùå Cannot perform attribution - final version not found\")\n",
        "            return None\n",
        "\n",
        "        final_sentences = processed_versions['final']['sentences']\n",
        "        sentence_attributions = []\n",
        "\n",
        "        print(f\"  üìù Tracing {len(final_sentences)} final sentences...\")\n",
        "\n",
        "        for final_idx, final_sentence in enumerate(final_sentences):\n",
        "            attribution = {\n",
        "                'final_index': final_idx,\n",
        "                'final_sentence': final_sentence,\n",
        "                'origin_version': None,\n",
        "                'origin_index': None,\n",
        "                'similarity_scores': {},\n",
        "                'modification_path': []\n",
        "            }\n",
        "\n",
        "            # Check each previous version (in reverse order to find earliest origin)\n",
        "            for version in reversed(available_versions[:-1]):  # Exclude 'final'\n",
        "                version_sentences = processed_versions[version]['sentences']\n",
        "\n",
        "                best_match = {'index': -1, 'similarity': 0, 'sentence': ''}\n",
        "\n",
        "                for sent_idx, version_sentence in enumerate(version_sentences):\n",
        "                    # Calculate similarity\n",
        "                    lexical = self._quick_lexical_similarity(final_sentence, version_sentence)\n",
        "                    semantic = self._quick_semantic_similarity(final_sentence, version_sentence)\n",
        "                    combined = (lexical + semantic) / 2\n",
        "\n",
        "                    if combined > best_match['similarity']:\n",
        "                        best_match = {\n",
        "                            'index': sent_idx,\n",
        "                            'similarity': combined,\n",
        "                            'sentence': version_sentence\n",
        "                        }\n",
        "\n",
        "                attribution['similarity_scores'][version] = best_match['similarity']\n",
        "\n",
        "                # If similarity is above threshold, this could be the origin\n",
        "                if best_match['similarity'] >= self.similarity_threshold:\n",
        "                    if attribution['origin_version'] is None:  # First match found (earliest version)\n",
        "                        attribution['origin_version'] = version\n",
        "                        attribution['origin_index'] = best_match['index']\n",
        "\n",
        "                    attribution['modification_path'].append({\n",
        "                        'version': version,\n",
        "                        'similarity': best_match['similarity'],\n",
        "                        'sentence': best_match['sentence']\n",
        "                    })\n",
        "\n",
        "            # If no origin found, it's new content\n",
        "            if attribution['origin_version'] is None:\n",
        "                attribution['origin_version'] = 'new_in_final'\n",
        "\n",
        "            sentence_attributions.append(attribution)\n",
        "\n",
        "        return sentence_attributions\n",
        "\n",
        "    def _quick_lexical_similarity(self, text1, text2):\n",
        "        \"\"\"Quick lexical similarity calculation\"\"\"\n",
        "        words1 = set(text1.lower().split())\n",
        "        words2 = set(text2.lower().split())\n",
        "        if not words1 and not words2:\n",
        "            return 1.0\n",
        "        if not words1 or not words2:\n",
        "            return 0.0\n",
        "        return len(words1.intersection(words2)) / len(words1.union(words2))\n",
        "\n",
        "    def _quick_semantic_similarity(self, text1, text2):\n",
        "        \"\"\"Quick semantic similarity calculation\"\"\"\n",
        "        embeddings = semantic_model.encode([text1, text2])\n",
        "        return float(cosine_similarity([embeddings[0]], [embeddings[1]])[0][0])\n",
        "\n",
        "    def calculate_attribution_statistics(self, sentence_attributions):\n",
        "        \"\"\"Calculate overall attribution statistics\"\"\"\n",
        "        total_sentences = len(sentence_attributions)\n",
        "\n",
        "        # Count by origin version\n",
        "        origin_counts = defaultdict(int)\n",
        "        for attribution in sentence_attributions:\n",
        "            origin_counts[attribution['origin_version']] += 1\n",
        "\n",
        "        # Calculate percentages\n",
        "        origin_percentages = {}\n",
        "        for version, count in origin_counts.items():\n",
        "            origin_percentages[version] = {\n",
        "                'count': count,\n",
        "                'percentage': (count / total_sentences) * 100\n",
        "            }\n",
        "\n",
        "        # Calculate modification statistics\n",
        "        modification_stats = {\n",
        "            'high_similarity': 0,  # >0.8\n",
        "            'medium_similarity': 0,  # 0.5-0.8\n",
        "            'low_similarity': 0,  # 0.3-0.5\n",
        "            'new_content': 0  # <0.3 or new_in_final\n",
        "        }\n",
        "\n",
        "        for attribution in sentence_attributions:\n",
        "            if attribution['origin_version'] == 'new_in_final':\n",
        "                modification_stats['new_content'] += 1\n",
        "            else:\n",
        "                # Get highest similarity score\n",
        "                max_similarity = max(attribution['similarity_scores'].values()) if attribution['similarity_scores'] else 0\n",
        "\n",
        "                if max_similarity > 0.8:\n",
        "                    modification_stats['high_similarity'] += 1\n",
        "                elif max_similarity > 0.5:\n",
        "                    modification_stats['medium_similarity'] += 1\n",
        "                elif max_similarity > 0.3:\n",
        "                    modification_stats['low_similarity'] += 1\n",
        "                else:\n",
        "                    modification_stats['new_content'] += 1\n",
        "\n",
        "        # Convert to percentages\n",
        "        modification_percentages = {}\n",
        "        for category, count in modification_stats.items():\n",
        "            modification_percentages[category] = {\n",
        "                'count': count,\n",
        "                'percentage': (count / total_sentences) * 100\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'total_sentences': total_sentences,\n",
        "            'origin_distribution': origin_percentages,\n",
        "            'modification_distribution': modification_percentages\n",
        "        }\n",
        "\n",
        "    def analyze_attribution(self, processed_versions, similarity_results):\n",
        "        \"\"\"Perform complete attribution analysis\"\"\"\n",
        "        sentence_attributions = self.trace_sentence_origins(processed_versions, similarity_results)\n",
        "\n",
        "        if sentence_attributions is None:\n",
        "            return None\n",
        "\n",
        "        attribution_statistics = self.calculate_attribution_statistics(sentence_attributions)\n",
        "\n",
        "        self.attribution_results = {\n",
        "            'sentence_attributions': sentence_attributions,\n",
        "            'statistics': attribution_statistics,\n",
        "            'analysis_timestamp': datetime.now().isoformat(),\n",
        "            'similarity_threshold': self.similarity_threshold\n",
        "        }\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"\\nüìä Attribution Summary:\")\n",
        "        print(f\"  Total sentences in final: {attribution_statistics['total_sentences']}\")\n",
        "\n",
        "        print(f\"\\n  Origin Distribution:\")\n",
        "        for version, data in attribution_statistics['origin_distribution'].items():\n",
        "            print(f\"    {version}: {data['count']} sentences ({data['percentage']:.1f}%)\")\n",
        "\n",
        "        print(f\"\\n  Modification Levels:\")\n",
        "        for category, data in attribution_statistics['modification_distribution'].items():\n",
        "            print(f\"    {category}: {data['count']} sentences ({data['percentage']:.1f}%)\")\n",
        "\n",
        "        return self.attribution_results\n",
        "\n",
        "print(\"üìç AttributionMapper class loaded. Ready for attribution analysis.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIDJ9C50MrQ3",
        "outputId": "cb07aed3-0389-4fc1-83d5-83547ae2c5d5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìç AttributionMapper class loaded. Ready for attribution analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 4: COMBINED EXECUTION FUNCTION\n",
        "# =============================================================================\n",
        "\n",
        "def run_steps_3_4(article_versions, preprocessor, output_path):\n",
        "    \"\"\"Run steps 3-4: Similarity Analysis and Attribution Mapping\"\"\"\n",
        "    print(f\"üöÄ Starting Steps 3-4 for article: {article_versions.article_name}\")\n",
        "\n",
        "    # Step 3: Similarity Analysis\n",
        "    similarity_analyzer = SimilarityAnalyzer(semantic_model)\n",
        "    similarity_results = similarity_analyzer.analyze_all_versions(preprocessor.processed_versions)\n",
        "\n",
        "    # Step 4: Attribution Mapping\n",
        "    attribution_mapper = AttributionMapper(similarity_threshold=0.3)\n",
        "    attribution_results = attribution_mapper.analyze_attribution(\n",
        "        preprocessor.processed_versions,\n",
        "        similarity_results\n",
        "    )\n",
        "\n",
        "    # Combine results\n",
        "    combined_results = {\n",
        "        'article_name': article_versions.article_name,\n",
        "        'analysis_timestamp': datetime.now().isoformat(),\n",
        "        'article_metadata': article_versions.metadata,\n",
        "        'processing_summary': preprocessor.get_processing_summary(),\n",
        "        'similarity_analysis': similarity_results,\n",
        "        'attribution_analysis': attribution_results\n",
        "    }\n",
        "\n",
        "    # Save comprehensive results\n",
        "    results_file = f\"{output_path}/{article_versions.article_name}_complete_analysis.json\"\n",
        "    with open(results_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(combined_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\nüíæ Complete analysis saved: {results_file}\")\n",
        "\n",
        "    # Generate summary metrics for article footer\n",
        "    footer_metrics = generate_footer_metrics(combined_results)\n",
        "\n",
        "    # Save footer metrics separately\n",
        "    footer_file = f\"{output_path}/{article_versions.article_name}_footer_metrics.json\"\n",
        "    with open(footer_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(footer_metrics, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"üìä Footer metrics saved: {footer_file}\")\n",
        "\n",
        "    return combined_results, footer_metrics\n",
        "\n",
        "def generate_footer_metrics(combined_results):\n",
        "    \"\"\"Generate clean metrics for article footer\"\"\"\n",
        "\n",
        "    # Get processing stats\n",
        "    processing = combined_results['processing_summary']\n",
        "\n",
        "    # Get attribution stats\n",
        "    if combined_results['attribution_analysis']:\n",
        "        attribution = combined_results['attribution_analysis']['statistics']\n",
        "        origin_dist = attribution['origin_distribution']\n",
        "        modification_dist = attribution['modification_distribution']\n",
        "    else:\n",
        "        origin_dist = {}\n",
        "        modification_dist = {}\n",
        "\n",
        "    # Get similarity stats (draft to final)\n",
        "    draft_to_final = combined_results['similarity_analysis']['draft_to_final']\n",
        "    overall_similarity = draft_to_final['full_text']['combined'] if draft_to_final else 0\n",
        "\n",
        "    footer_metrics = {\n",
        "        'article_name': combined_results['article_name'],\n",
        "        'word_progression': {\n",
        "            'draft': processing.get('draft', {}).get('word_count', 0),\n",
        "            'final': processing.get('final', {}).get('word_count', 0),\n",
        "            'change_percentage': 0\n",
        "        },\n",
        "        'content_retention': {\n",
        "            'overall_similarity': round(overall_similarity * 100, 1),\n",
        "            'content_origins': {}\n",
        "        },\n",
        "        'modification_summary': {},\n",
        "        'generated_at': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Calculate word change percentage\n",
        "    if footer_metrics['word_progression']['draft'] > 0:\n",
        "        draft_words = footer_metrics['word_progression']['draft']\n",
        "        final_words = footer_metrics['word_progression']['final']\n",
        "        change = ((final_words - draft_words) / draft_words) * 100\n",
        "        footer_metrics['word_progression']['change_percentage'] = round(change, 1)\n",
        "\n",
        "    # Simplify origin distribution for footer\n",
        "    for version, data in origin_dist.items():\n",
        "        if version != 'new_in_final':\n",
        "            footer_metrics['content_retention']['content_origins'][version] = round(data['percentage'], 1)\n",
        "\n",
        "    # Simplify modification distribution\n",
        "    for category, data in modification_dist.items():\n",
        "        clean_category = category.replace('_', ' ').title()\n",
        "        footer_metrics['modification_summary'][clean_category] = round(data['percentage'], 1)\n",
        "\n",
        "    return footer_metrics\n",
        "\n",
        "print(\"üéØ Execution functions loaded. Ready to run complete analysis!\")\n",
        "print(\"\\nTo run the complete analysis:\")\n",
        "print(\"combined_results, footer_metrics = run_steps_3_4(article_versions, preprocessor, 'your_output_path')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbD5vnmTMvo5",
        "outputId": "6c8f2086-8f30-428c-b528-3eb9ed4a7e3d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Execution functions loaded. Ready to run complete analysis!\n",
            "\n",
            "To run the complete analysis:\n",
            "combined_results, footer_metrics = run_steps_3_4(article_versions, preprocessor, 'your_output_path')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 5: QUICK EXECUTION FOR EXISTING DATA\n",
        "# =============================================================================\n",
        "\n",
        "def run_complete_analysis_from_existing(article_versions, preprocessor):\n",
        "    \"\"\"Run steps 3-4 using the output path from existing data\"\"\"\n",
        "    # Find where the Step 1-2 checkpoint was actually saved\n",
        "    base_path = article_versions.input_path\n",
        "\n",
        "    # Check for the nested output structure that was created in Steps 1-2\n",
        "    nested_output_path = os.path.join(base_path, 'output', 'output')\n",
        "    regular_output_path = os.path.join(base_path, 'output')\n",
        "\n",
        "    # Use the path where the checkpoint file exists\n",
        "    checkpoint_file = f\"markup-languages_checkpoint_steps_1_2.json\"\n",
        "\n",
        "    if os.path.exists(os.path.join(nested_output_path, checkpoint_file)):\n",
        "        output_path = nested_output_path\n",
        "        print(f\"üìÇ Using nested output path: {output_path}\")\n",
        "    elif os.path.exists(os.path.join(regular_output_path, checkpoint_file)):\n",
        "        output_path = regular_output_path\n",
        "        print(f\"üìÇ Using regular output path: {output_path}\")\n",
        "    else:\n",
        "        # Create regular output path as fallback\n",
        "        output_path = regular_output_path\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "        print(f\"üìÇ Created output path: {output_path}\")\n",
        "\n",
        "    return run_steps_3_4(article_versions, preprocessor, output_path)\n",
        "\n",
        "print(\"‚ö° Quick execution function available:\")\n",
        "print(\"combined_results, footer_metrics = run_complete_analysis_from_existing(article_versions, preprocessor)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7yqzZneMzgn",
        "outputId": "d7e2bc32-ac5f-441d-a469-e40a026f970c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° Quick execution function available:\n",
            "combined_results, footer_metrics = run_complete_analysis_from_existing(article_versions, preprocessor)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_results, footer_metrics = run_complete_analysis_from_existing(article_versions, preprocessor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYliXyeZM17X",
        "outputId": "1501de5f-6c26-4d78-dfc8-d8ec8f5ea728"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Using nested output path: /content/drive/MyDrive/Google Drive/syntaxandempathy/30-articles/ai-vs-human/output/output\n",
            "üöÄ Starting Steps 3-4 for article: markup-languages\n",
            "\n",
            "üîç Step 3: Similarity Analysis\n",
            "  üîç Analyzing draft ‚Üí refined\n",
            "    Analyzing 47 vs 60 sentences...\n",
            "  üîç Analyzing refined ‚Üí edited\n",
            "    Analyzing 60 vs 45 sentences...\n",
            "  üîç Analyzing edited ‚Üí final\n",
            "    Analyzing 45 vs 42 sentences...\n",
            "  üîç Analyzing draft ‚Üí final (overall change)\n",
            "  üîç Analyzing draft ‚Üí final\n",
            "    Analyzing 47 vs 42 sentences...\n",
            "\n",
            "üìç Step 4: Attribution Mapping\n",
            "  üìù Tracing 42 final sentences...\n",
            "\n",
            "üìä Attribution Summary:\n",
            "  Total sentences in final: 42\n",
            "\n",
            "  Origin Distribution:\n",
            "    edited: 36 sentences (85.7%)\n",
            "    draft: 1 sentences (2.4%)\n",
            "    new_in_final: 5 sentences (11.9%)\n",
            "\n",
            "  Modification Levels:\n",
            "    high_similarity: 3 sentences (7.1%)\n",
            "    medium_similarity: 17 sentences (40.5%)\n",
            "    low_similarity: 17 sentences (40.5%)\n",
            "    new_content: 5 sentences (11.9%)\n",
            "\n",
            "üíæ Complete analysis saved: /content/drive/MyDrive/Google Drive/syntaxandempathy/30-articles/ai-vs-human/output/output/markup-languages_complete_analysis.json\n",
            "üìä Footer metrics saved: /content/drive/MyDrive/Google Drive/syntaxandempathy/30-articles/ai-vs-human/output/output/markup-languages_footer_metrics.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 1: TREND ANALYSIS SETUP\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import glob\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Archive configuration\n",
        "ARCHIVE_BASE_PATH = '/content/drive/MyDrive/Google Drive/syntaxandempathy/99-past/'\n",
        "\n",
        "class TrendAnalyzer:\n",
        "    \"\"\"Class to analyze trends across multiple articles over time periods\"\"\"\n",
        "\n",
        "    def __init__(self, archive_path=ARCHIVE_BASE_PATH):\n",
        "        self.archive_path = archive_path\n",
        "        self.found_articles = []\n",
        "        self.period_data = {}\n",
        "        self.trend_results = {}\n",
        "\n",
        "    def parse_period(self, period):\n",
        "        \"\"\"Parse period string into date patterns\"\"\"\n",
        "        period_patterns = []\n",
        "\n",
        "        if len(period) == 4:  # Year: \"2025\"\n",
        "            # All months in the year: 202501*, 202502*, etc.\n",
        "            for month in range(1, 13):\n",
        "                pattern = f\"{period}{month:02d}*\"\n",
        "                period_patterns.append(pattern)\n",
        "\n",
        "        elif len(period) == 7 and period[4] == '-':  # Month: \"2025-01\"\n",
        "            year, month = period.split('-')\n",
        "            pattern = f\"{year}{month}*\"  # e.g., \"202501*\"\n",
        "            period_patterns.append(pattern)\n",
        "\n",
        "        elif period.endswith(('Q1', 'Q2', 'Q3', 'Q4')):  # Quarter: \"2025-Q1\"\n",
        "            year, quarter = period.split('-')\n",
        "            quarter_months = {\n",
        "                'Q1': ['01', '02', '03'],\n",
        "                'Q2': ['04', '05', '06'],\n",
        "                'Q3': ['07', '08', '09'],\n",
        "                'Q4': ['10', '11', '12']\n",
        "            }\n",
        "\n",
        "            for month in quarter_months[quarter]:\n",
        "                pattern = f\"{year}{month}*\"  # e.g., \"202501*\", \"202502*\", \"202503*\"\n",
        "                period_patterns.append(pattern)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid period format: {period}. Use 'YYYY', 'YYYY-MM', or 'YYYY-Q#'\")\n",
        "\n",
        "        return period_patterns\n",
        "\n",
        "    def find_articles_for_period(self, period):\n",
        "        \"\"\"Find all articles matching the period\"\"\"\n",
        "        print(f\"üîç Searching for articles in period: {period}\")\n",
        "        print(f\"üìÇ Archive path: {self.archive_path}\")\n",
        "\n",
        "        if not os.path.exists(self.archive_path):\n",
        "            print(f\"‚ùå Archive path does not exist: {self.archive_path}\")\n",
        "            return []\n",
        "\n",
        "        patterns = self.parse_period(period)\n",
        "        found_articles = []\n",
        "\n",
        "        # Get all directories in archive\n",
        "        all_dirs = [d for d in os.listdir(self.archive_path)\n",
        "                   if os.path.isdir(os.path.join(self.archive_path, d))]\n",
        "\n",
        "        print(f\"üìÅ Found {len(all_dirs)} directories in archive\")\n",
        "\n",
        "        # Match directories against patterns\n",
        "        for pattern in patterns:\n",
        "            # Convert shell pattern to regex: 202506* becomes ^202506.*\n",
        "            pattern_regex = f\"^{pattern.replace('*', '.*')}\"\n",
        "            regex = re.compile(pattern_regex)\n",
        "\n",
        "            for dir_name in all_dirs:\n",
        "                if regex.match(dir_name):\n",
        "                    article_path = os.path.join(self.archive_path, dir_name)\n",
        "\n",
        "                    # Look for analysis file\n",
        "                    analysis_files = glob.glob(os.path.join(article_path, 'output', '*_complete_analysis.json'))\n",
        "\n",
        "                    if analysis_files:\n",
        "                        # Avoid duplicates\n",
        "                        if not any(item['folder_name'] == dir_name for item in found_articles):\n",
        "                            found_articles.append({\n",
        "                                'folder_name': dir_name,\n",
        "                                'article_path': article_path,\n",
        "                                'analysis_file': analysis_files[0],\n",
        "                                'date_prefix': dir_name[:8],  # Extract YYYYMMDD\n",
        "                                'article_name': dir_name[9:]  # Extract name after date-\n",
        "                            })\n",
        "                            print(f\"  ‚úì Found: {dir_name} (matches {pattern})\")\n",
        "                    else:\n",
        "                        print(f\"  ‚ö† No analysis file found in: {dir_name}\")\n",
        "\n",
        "        found_articles.sort(key=lambda x: x['date_prefix'])  # Sort by date\n",
        "        self.found_articles = found_articles\n",
        "\n",
        "        print(f\"üìä Total articles found for {period}: {len(found_articles)}\")\n",
        "        return found_articles\n",
        "\n",
        "    def load_article_data(self, article_info):\n",
        "        \"\"\"Load analysis data for a single article\"\"\"\n",
        "        try:\n",
        "            with open(article_info['analysis_file'], 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # Add metadata\n",
        "            data['folder_name'] = article_info['folder_name']\n",
        "            data['publication_date'] = article_info['date_prefix']\n",
        "\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading {article_info['analysis_file']}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def aggregate_attribution_trends(self, articles_data):\n",
        "        \"\"\"Aggregate attribution data across articles\"\"\"\n",
        "        attribution_trends = {\n",
        "            'by_article': [],\n",
        "            'averages': {},\n",
        "            'totals': {},\n",
        "            'trends_over_time': []\n",
        "        }\n",
        "\n",
        "        # Collect data for each article\n",
        "        for article in articles_data:\n",
        "            if not article or 'attribution_analysis' not in article:\n",
        "                continue\n",
        "\n",
        "            attr_stats = article['attribution_analysis']['statistics']\n",
        "            origin_dist = attr_stats['origin_distribution']\n",
        "            mod_dist = attr_stats['modification_distribution']\n",
        "\n",
        "            article_data = {\n",
        "                'article_name': article['article_name'],\n",
        "                'publication_date': article['publication_date'],\n",
        "                'total_sentences': attr_stats['total_sentences'],\n",
        "                'origin_percentages': {},\n",
        "                'modification_percentages': {}\n",
        "            }\n",
        "\n",
        "            # Extract origin percentages\n",
        "            for version, data in origin_dist.items():\n",
        "                article_data['origin_percentages'][version] = data['percentage']\n",
        "\n",
        "            # Extract modification percentages\n",
        "            for category, data in mod_dist.items():\n",
        "                article_data['modification_percentages'][category] = data['percentage']\n",
        "\n",
        "            attribution_trends['by_article'].append(article_data)\n",
        "\n",
        "        # Calculate averages across all articles\n",
        "        if attribution_trends['by_article']:\n",
        "            all_origins = set()\n",
        "            all_modifications = set()\n",
        "\n",
        "            for article in attribution_trends['by_article']:\n",
        "                all_origins.update(article['origin_percentages'].keys())\n",
        "                all_modifications.update(article['modification_percentages'].keys())\n",
        "\n",
        "            # Average origin percentages\n",
        "            for origin in all_origins:\n",
        "                values = [a['origin_percentages'].get(origin, 0) for a in attribution_trends['by_article']]\n",
        "                attribution_trends['averages'][f'origin_{origin}'] = np.mean(values)\n",
        "\n",
        "            # Average modification percentages\n",
        "            for mod in all_modifications:\n",
        "                values = [a['modification_percentages'].get(mod, 0) for a in attribution_trends['by_article']]\n",
        "                attribution_trends['averages'][f'modification_{mod}'] = np.mean(values)\n",
        "\n",
        "        return attribution_trends\n",
        "\n",
        "    def aggregate_similarity_trends(self, articles_data):\n",
        "        \"\"\"Aggregate similarity data across articles\"\"\"\n",
        "        similarity_trends = {\n",
        "            'draft_to_final': [],\n",
        "            'sequential_changes': [],\n",
        "            'averages': {}\n",
        "        }\n",
        "\n",
        "        for article in articles_data:\n",
        "            if not article or 'similarity_analysis' not in article:\n",
        "                continue\n",
        "\n",
        "            sim_analysis = article['similarity_analysis']\n",
        "\n",
        "            # Draft to final similarity\n",
        "            if sim_analysis['draft_to_final']:\n",
        "                draft_final = sim_analysis['draft_to_final']['full_text']\n",
        "                similarity_trends['draft_to_final'].append({\n",
        "                    'article_name': article['article_name'],\n",
        "                    'publication_date': article['publication_date'],\n",
        "                    'lexical_similarity': draft_final['lexical']['lexical_average'],\n",
        "                    'semantic_similarity': draft_final['semantic']['semantic_similarity'],\n",
        "                    'combined_similarity': draft_final['combined']\n",
        "                })\n",
        "\n",
        "            # Sequential changes\n",
        "            for seq in sim_analysis['sequential_analysis']:\n",
        "                similarity_trends['sequential_changes'].append({\n",
        "                    'article_name': article['article_name'],\n",
        "                    'publication_date': article['publication_date'],\n",
        "                    'version_pair': seq['version_pair'],\n",
        "                    'combined_similarity': seq['full_text']['combined']\n",
        "                })\n",
        "\n",
        "        # Calculate averages\n",
        "        if similarity_trends['draft_to_final']:\n",
        "            df_sims = similarity_trends['draft_to_final']\n",
        "            similarity_trends['averages']['draft_to_final'] = {\n",
        "                'lexical': np.mean([s['lexical_similarity'] for s in df_sims]),\n",
        "                'semantic': np.mean([s['semantic_similarity'] for s in df_sims]),\n",
        "                'combined': np.mean([s['combined_similarity'] for s in df_sims])\n",
        "            }\n",
        "\n",
        "        return similarity_trends\n",
        "\n",
        "    def aggregate_word_count_trends(self, articles_data):\n",
        "        \"\"\"Aggregate word count progression across articles\"\"\"\n",
        "        word_trends = {\n",
        "            'by_article': [],\n",
        "            'averages': {},\n",
        "            'progression_patterns': []\n",
        "        }\n",
        "\n",
        "        for article in articles_data:\n",
        "            if not article or 'processing_summary' not in article:\n",
        "                continue\n",
        "\n",
        "            processing = article['processing_summary']\n",
        "\n",
        "            article_words = {\n",
        "                'article_name': article['article_name'],\n",
        "                'publication_date': article['publication_date'],\n",
        "                'word_counts': {},\n",
        "                'progression': []\n",
        "            }\n",
        "\n",
        "            # Extract word counts for each version\n",
        "            version_order = ['draft', 'refined', 'edited', 'final']\n",
        "            for version in version_order:\n",
        "                if version in processing:\n",
        "                    count = processing[version]['word_count']\n",
        "                    article_words['word_counts'][version] = count\n",
        "                    article_words['progression'].append(count)\n",
        "\n",
        "            word_trends['by_article'].append(article_words)\n",
        "\n",
        "        # Calculate average progressions\n",
        "        if word_trends['by_article']:\n",
        "            version_order = ['draft', 'refined', 'edited', 'final']\n",
        "            for version in version_order:\n",
        "                counts = [a['word_counts'].get(version, 0) for a in word_trends['by_article'] if version in a['word_counts']]\n",
        "                if counts:\n",
        "                    word_trends['averages'][version] = np.mean(counts)\n",
        "\n",
        "        return word_trends\n",
        "\n",
        "    def analyze_trends(self, period):\n",
        "        \"\"\"Perform complete trend analysis for a period\"\"\"\n",
        "        print(f\"\\nüöÄ Starting trend analysis for period: {period}\")\n",
        "\n",
        "        # Find articles\n",
        "        articles = self.find_articles_for_period(period)\n",
        "\n",
        "        if not articles:\n",
        "            print(f\"‚ùå No articles found for period {period}\")\n",
        "            return None\n",
        "\n",
        "        # Load article data\n",
        "        print(f\"\\nüìñ Loading analysis data for {len(articles)} articles...\")\n",
        "        articles_data = []\n",
        "\n",
        "        for article_info in articles:\n",
        "            data = self.load_article_data(article_info)\n",
        "            if data:\n",
        "                articles_data.append(data)\n",
        "                print(f\"  ‚úì Loaded: {article_info['article_name']}\")\n",
        "            else:\n",
        "                print(f\"  ‚úó Failed: {article_info['article_name']}\")\n",
        "\n",
        "        print(f\"üìä Successfully loaded {len(articles_data)} articles\")\n",
        "\n",
        "        # Perform trend analysis\n",
        "        print(f\"\\nüîç Analyzing trends...\")\n",
        "\n",
        "        attribution_trends = self.aggregate_attribution_trends(articles_data)\n",
        "        similarity_trends = self.aggregate_similarity_trends(articles_data)\n",
        "        word_count_trends = self.aggregate_word_count_trends(articles_data)\n",
        "\n",
        "        # Compile results\n",
        "        self.trend_results = {\n",
        "            'period': period,\n",
        "            'analysis_timestamp': datetime.now().isoformat(),\n",
        "            'articles_analyzed': len(articles_data),\n",
        "            'article_list': [{'name': a['article_name'], 'date': a['publication_date']} for a in articles_data],\n",
        "            'attribution_trends': attribution_trends,\n",
        "            'similarity_trends': similarity_trends,\n",
        "            'word_count_trends': word_count_trends,\n",
        "            'summary': self.generate_trend_summary(attribution_trends, similarity_trends, word_count_trends)\n",
        "        }\n",
        "\n",
        "        return self.trend_results\n",
        "\n",
        "    def generate_trend_summary(self, attribution_trends, similarity_trends, word_count_trends):\n",
        "        \"\"\"Generate a summary of key trends\"\"\"\n",
        "        summary = {\n",
        "            'key_metrics': {},\n",
        "            'patterns': [],\n",
        "            'insights': []\n",
        "        }\n",
        "\n",
        "        # Key attribution metrics\n",
        "        if attribution_trends['averages']:\n",
        "            avg_draft = attribution_trends['averages'].get('origin_draft', 0)\n",
        "            avg_edited = attribution_trends['averages'].get('origin_edited', 0)\n",
        "            avg_new = attribution_trends['averages'].get('origin_new_in_final', 0)\n",
        "\n",
        "            summary['key_metrics']['avg_draft_retention'] = round(avg_draft, 1)\n",
        "            summary['key_metrics']['avg_edited_dominance'] = round(avg_edited, 1)\n",
        "            summary['key_metrics']['avg_new_content'] = round(avg_new, 1)\n",
        "\n",
        "        # Key similarity metrics\n",
        "        if similarity_trends['averages'].get('draft_to_final'):\n",
        "            df_sim = similarity_trends['averages']['draft_to_final']\n",
        "            summary['key_metrics']['avg_draft_final_similarity'] = round(df_sim['combined'] * 100, 1)\n",
        "\n",
        "        # Word count patterns\n",
        "        if word_count_trends['averages']:\n",
        "            avg_draft = word_count_trends['averages'].get('draft', 0)\n",
        "            avg_final = word_count_trends['averages'].get('final', 0)\n",
        "            if avg_draft > 0:\n",
        "                change_pct = ((avg_final - avg_draft) / avg_draft) * 100\n",
        "                summary['key_metrics']['avg_word_change_pct'] = round(change_pct, 1)\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def save_trend_analysis(self, output_path, filename_prefix=\"trend_analysis\"):\n",
        "        \"\"\"Save trend analysis results\"\"\"\n",
        "        if not self.trend_results:\n",
        "            print(\"‚ùå No trend results to save\")\n",
        "            return None\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        # Save comprehensive results\n",
        "        results_file = os.path.join(output_path, f\"{filename_prefix}_{self.trend_results['period']}.json\")\n",
        "\n",
        "        with open(results_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.trend_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"üíæ Trend analysis saved: {results_file}\")\n",
        "\n",
        "        # Also save a summary report\n",
        "        summary_file = os.path.join(output_path, f\"{filename_prefix}_{self.trend_results['period']}_summary.json\")\n",
        "\n",
        "        summary_data = {\n",
        "            'period': self.trend_results['period'],\n",
        "            'articles_count': self.trend_results['articles_analyzed'],\n",
        "            'key_metrics': self.trend_results['summary']['key_metrics'],\n",
        "            'article_list': self.trend_results['article_list']\n",
        "        }\n",
        "\n",
        "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(summary_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"üìã Summary report saved: {summary_file}\")\n",
        "\n",
        "        return results_file, summary_file\n",
        "\n",
        "print(\"üìà TrendAnalyzer class loaded. Ready for trend analysis!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vnKEAt-eglI",
        "outputId": "f5e6e7d9-7702-49e8-c69d-54f8480f02dd"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìà TrendAnalyzer class loaded. Ready for trend analysis!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 2: EXECUTION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_period_trends(period, output_path=None):\n",
        "    \"\"\"Analyze trends for a specific period\"\"\"\n",
        "    print(f\"üìà Analyzing trends for period: {period}\")\n",
        "\n",
        "    # Create analyzer\n",
        "    analyzer = TrendAnalyzer()\n",
        "\n",
        "    # Run analysis\n",
        "    results = analyzer.analyze_trends(period)\n",
        "\n",
        "    if not results:\n",
        "        return None\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\nüìä TREND ANALYSIS SUMMARY for {period}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    summary = results['summary']['key_metrics']\n",
        "\n",
        "    if 'avg_draft_retention' in summary:\n",
        "        print(f\"üìù Average content retention from draft: {summary['avg_draft_retention']}%\")\n",
        "\n",
        "    if 'avg_edited_dominance' in summary:\n",
        "        print(f\"‚úèÔ∏è  Average content from editing phase: {summary['avg_edited_dominance']}%\")\n",
        "\n",
        "    if 'avg_new_content' in summary:\n",
        "        print(f\"üÜï Average new content in final: {summary['avg_new_content']}%\")\n",
        "\n",
        "    if 'avg_draft_final_similarity' in summary:\n",
        "        print(f\"üîÑ Average draft-to-final similarity: {summary['avg_draft_final_similarity']}%\")\n",
        "\n",
        "    if 'avg_word_change_pct' in summary:\n",
        "        change = summary['avg_word_change_pct']\n",
        "        direction = \"increase\" if change > 0 else \"decrease\"\n",
        "        print(f\"üìè Average word count {direction}: {abs(change):.1f}%\")\n",
        "\n",
        "    print(f\"\\nüìö Articles analyzed: {results['articles_analyzed']}\")\n",
        "    for article in results['article_list']:\n",
        "        print(f\"  ‚Ä¢ {article['date']}: {article['name']}\")\n",
        "\n",
        "    # Save results if output path provided\n",
        "    if output_path:\n",
        "        analyzer.save_trend_analysis(output_path)\n",
        "\n",
        "    return results, analyzer\n",
        "\n",
        "def quick_trend_check(period):\n",
        "    \"\"\"Quick trend check without saving files\"\"\"\n",
        "    results, analyzer = analyze_period_trends(period)\n",
        "    return results\n",
        "\n",
        "print(\"üéØ Execution functions loaded!\")\n",
        "print(\"\\nUsage examples:\")\n",
        "print(\"# Quick check (no files saved)\")\n",
        "print(\"results = quick_trend_check('2025-Q1')\")\n",
        "print(\"\\n# Full analysis with file output\")\n",
        "print(\"results, analyzer = analyze_period_trends('2025-01', '/your/output/path')\")\n",
        "print(\"\\n# Supported period formats:\")\n",
        "print(\"  ‚Ä¢ '2025' (full year)\")\n",
        "print(\"  ‚Ä¢ '2025-01' (specific month)\")\n",
        "print(\"  ‚Ä¢ '2025-Q1' (quarter)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKz52N5oel4w",
        "outputId": "587bf0c2-b401-4588-8ead-966606750dd7"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Execution functions loaded!\n",
            "\n",
            "Usage examples:\n",
            "# Quick check (no files saved)\n",
            "results = quick_trend_check('2025-Q1')\n",
            "\n",
            "# Full analysis with file output\n",
            "results, analyzer = analyze_period_trends('2025-01', '/your/output/path')\n",
            "\n",
            "# Supported period formats:\n",
            "  ‚Ä¢ '2025' (full year)\n",
            "  ‚Ä¢ '2025-01' (specific month)\n",
            "  ‚Ä¢ '2025-Q1' (quarter)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 3: TESTING AND VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "def test_archive_structure():\n",
        "    \"\"\"Test the archive structure and show what's available\"\"\"\n",
        "    print(f\"üîç Testing archive structure...\")\n",
        "    print(f\"üìÇ Archive path: {ARCHIVE_BASE_PATH}\")\n",
        "\n",
        "    if not os.path.exists(ARCHIVE_BASE_PATH):\n",
        "        print(f\"‚ùå Archive path does not exist!\")\n",
        "        return False\n",
        "\n",
        "    # List all directories\n",
        "    all_dirs = [d for d in os.listdir(ARCHIVE_BASE_PATH)\n",
        "               if os.path.isdir(os.path.join(ARCHIVE_BASE_PATH, d))]\n",
        "\n",
        "    print(f\"üìÅ Found {len(all_dirs)} directories:\")\n",
        "\n",
        "    # Look for date pattern: YYYYMMDD-*\n",
        "    date_pattern = re.compile(r'^\\d{8}-.*')\n",
        "    valid_dirs = []\n",
        "\n",
        "    for dir_name in sorted(all_dirs):\n",
        "        print(f\"  üîç Checking: '{dir_name}'\")\n",
        "\n",
        "        if date_pattern.match(dir_name):\n",
        "            # Check for analysis file\n",
        "            analysis_files = glob.glob(os.path.join(ARCHIVE_BASE_PATH, dir_name, 'output', '*_complete_analysis.json'))\n",
        "            status = \"‚úì\" if analysis_files else \"‚ö†\"\n",
        "            print(f\"    {status} MATCHES date pattern (YYYYMMDD-name)\")\n",
        "            if analysis_files:\n",
        "                valid_dirs.append(dir_name)\n",
        "                print(f\"      Found analysis file: {os.path.basename(analysis_files[0])}\")\n",
        "            else:\n",
        "                print(f\"      No analysis file in output/ folder\")\n",
        "        else:\n",
        "            print(f\"    ‚úó Does NOT match YYYYMMDD-name pattern\")\n",
        "\n",
        "    print(f\"\\nüìä Summary:\")\n",
        "    print(f\"  Total directories: {len(all_dirs)}\")\n",
        "    print(f\"  Valid date format: {len([d for d in all_dirs if date_pattern.match(d)])}\")\n",
        "    print(f\"  With analysis files: {len(valid_dirs)}\")\n",
        "\n",
        "    if valid_dirs:\n",
        "        print(f\"\\nüóìÔ∏è  Date range:\")\n",
        "        dates = [d[:8] for d in valid_dirs]\n",
        "        print(f\"  Earliest: {min(dates)}\")\n",
        "        print(f\"  Latest: {max(dates)}\")\n",
        "\n",
        "        # Show available periods\n",
        "        years = set(d[:4] for d in dates)\n",
        "        print(f\"\\nüìÖ Available periods you can analyze:\")\n",
        "        for year in sorted(years):\n",
        "            year_dates = [d for d in dates if d.startswith(year)]\n",
        "            months = set(d[4:6] for d in year_dates)\n",
        "            quarters = set(f\"Q{(int(d[4:6])-1)//3 + 1}\" for d in year_dates)\n",
        "            print(f\"  {year}: {len(year_dates)} articles\")\n",
        "            print(f\"    Months: {', '.join(sorted(months))}\")\n",
        "            print(f\"    Quarters: {', '.join(sorted(quarters))}\")\n",
        "\n",
        "    return len(valid_dirs) > 0\n",
        "\n",
        "print(\"üß™ Testing functions loaded!\")\n",
        "print(\"Run: test_archive_structure() to validate your archive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZt59BOgeqgc",
        "outputId": "082b56f0-6111-4e04-fa4c-52c97a3ae3bb"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing functions loaded!\n",
            "Run: test_archive_structure() to validate your archive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_archive_structure()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eefsNquHe-6r",
        "outputId": "4a5d74dc-4dd8-4299-aa4e-7259b600f2d4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Testing archive structure...\n",
            "üìÇ Archive path: /content/drive/MyDrive/Google Drive/syntaxandempathy/99-past/\n",
            "üìÅ Found 2 directories:\n",
            "  üîç Checking: '2024'\n",
            "    ‚úó Does NOT match YYYYMMDD-name pattern\n",
            "  üîç Checking: '20250614-ai-vs-human'\n",
            "    ‚úì MATCHES date pattern (YYYYMMDD-name)\n",
            "      Found analysis file: markup-languages_complete_analysis.json\n",
            "\n",
            "üìä Summary:\n",
            "  Total directories: 2\n",
            "  Valid date format: 1\n",
            "  With analysis files: 1\n",
            "\n",
            "üóìÔ∏è  Date range:\n",
            "  Earliest: 20250614\n",
            "  Latest: 20250614\n",
            "\n",
            "üìÖ Available periods you can analyze:\n",
            "  2025: 1 articles\n",
            "    Months: 06\n",
            "    Quarters: Q2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with the available periods\n",
        "results = quick_trend_check('2025-06')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2iCCyCXiYxV",
        "outputId": "cc7e7585-b702-431f-e14d-c8ea50f4c140"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìà Analyzing trends for period: 2025-06\n",
            "\n",
            "üöÄ Starting trend analysis for period: 2025-06\n",
            "üîç Searching for articles in period: 2025-06\n",
            "üìÇ Archive path: /content/drive/MyDrive/Google Drive/syntaxandempathy/99-past/\n",
            "üìÅ Found 2 directories in archive\n",
            "  ‚úì Found: 20250614-ai-vs-human (matches 202506*)\n",
            "üìä Total articles found for 2025-06: 1\n",
            "\n",
            "üìñ Loading analysis data for 1 articles...\n",
            "  ‚úì Loaded: ai-vs-human\n",
            "üìä Successfully loaded 1 articles\n",
            "\n",
            "üîç Analyzing trends...\n",
            "\n",
            "üìä TREND ANALYSIS SUMMARY for 2025-06\n",
            "==================================================\n",
            "üìù Average content retention from draft: 2.4%\n",
            "‚úèÔ∏è  Average content from editing phase: 85.7%\n",
            "üÜï Average new content in final: 11.9%\n",
            "üîÑ Average draft-to-final similarity: 43.1%\n",
            "üìè Average word count increase: 3.4%\n",
            "\n",
            "üìö Articles analyzed: 1\n",
            "  ‚Ä¢ 20250614: markup-languages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 1: VISUALIZATION SETUP AND DEPENDENCIES\n",
        "# =============================================================================\n",
        "\n",
        "# Install required packages\n",
        "!pip install matplotlib seaborn plotly pandas numpy -q\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.io as pio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "# Set up plotting styles\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "pio.templates.default = \"plotly_white\"\n",
        "\n",
        "# Custom color palette for consistency\n",
        "COLORS = {\n",
        "    'draft': '#FF6B6B',      # Red\n",
        "    'refined': '#4ECDC4',    # Teal\n",
        "    'edited': '#45B7D1',     # Blue\n",
        "    'final': '#96CEB4',      # Green\n",
        "    'new_content': '#FECA57', # Yellow\n",
        "    'high_similarity': '#48CAE4',    # Light blue\n",
        "    'medium_similarity': '#FFB3BA',  # Light pink\n",
        "    'low_similarity': '#FFDFBA',     # Light orange\n",
        "    'background': '#F8F9FA'\n",
        "}\n",
        "\n",
        "print(\"üìä Visualization dependencies loaded successfully!\")\n",
        "print(\"üé® Custom color palette configured\")\n",
        "\n",
        "class VisualizationEngine:\n",
        "    \"\"\"Unified visualization engine for individual articles and trend analysis\"\"\"\n",
        "\n",
        "    def __init__(self, output_path=None):\n",
        "        self.output_path = output_path\n",
        "        self.figures = {}\n",
        "\n",
        "    def create_content_flow_chart(self, data, title=\"Content Flow Analysis\", mode=\"individual\"):\n",
        "        \"\"\"Create a content flow visualization showing version progression\"\"\"\n",
        "\n",
        "        if mode == \"individual\":\n",
        "            # Individual article - show attribution percentages\n",
        "            attribution_stats = data['attribution_analysis']['statistics']\n",
        "            origin_dist = attribution_stats['origin_distribution']\n",
        "\n",
        "            # Prepare data for flow chart\n",
        "            flow_data = []\n",
        "            for version, stats in origin_dist.items():\n",
        "                if version != 'new_in_final':\n",
        "                    flow_data.append({\n",
        "                        'source': version.title(),\n",
        "                        'target': 'Final Article',\n",
        "                        'value': stats['percentage'],\n",
        "                        'count': stats['count']\n",
        "                    })\n",
        "\n",
        "            # Add new content\n",
        "            if 'new_in_final' in origin_dist:\n",
        "                flow_data.append({\n",
        "                    'source': 'New Content',\n",
        "                    'target': 'Final Article',\n",
        "                    'value': origin_dist['new_in_final']['percentage'],\n",
        "                    'count': origin_dist['new_in_final']['count']\n",
        "                })\n",
        "\n",
        "        else:\n",
        "            # Trend analysis - show average percentages\n",
        "            avg_data = data['attribution_trends']['averages']\n",
        "            flow_data = []\n",
        "\n",
        "            for key, value in avg_data.items():\n",
        "                if key.startswith('origin_') and not key.endswith('new_in_final'):\n",
        "                    version = key.replace('origin_', '').title()\n",
        "                    flow_data.append({\n",
        "                        'source': version,\n",
        "                        'target': 'Final Articles',\n",
        "                        'value': value,\n",
        "                        'count': f\"{value:.1f}% avg\"\n",
        "                    })\n",
        "\n",
        "            # Add new content average\n",
        "            if 'origin_new_in_final' in avg_data:\n",
        "                flow_data.append({\n",
        "                    'source': 'New Content',\n",
        "                    'target': 'Final Articles',\n",
        "                    'value': avg_data['origin_new_in_final'],\n",
        "                    'count': f\"{avg_data['origin_new_in_final']:.1f}% avg\"\n",
        "                })\n",
        "\n",
        "        # Create sankey-style visualization using matplotlib\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "        ax.set_xlim(0, 10)\n",
        "        ax.set_ylim(0, 10)\n",
        "\n",
        "        # Draw flow connections\n",
        "        y_positions = np.linspace(8, 2, len(flow_data))\n",
        "\n",
        "        for i, item in enumerate(flow_data):\n",
        "            # Source box\n",
        "            source_color = COLORS.get(item['source'].lower(), '#CCCCCC')\n",
        "            source_rect = patches.Rectangle((0.5, y_positions[i]-0.3), 2, 0.6,\n",
        "                                          facecolor=source_color, alpha=0.7, edgecolor='black')\n",
        "            ax.add_patch(source_rect)\n",
        "            ax.text(1.5, y_positions[i], item['source'], ha='center', va='center', fontweight='bold')\n",
        "\n",
        "            # Flow arrow\n",
        "            arrow_width = item['value'] / 100 * 0.4  # Scale arrow width by percentage\n",
        "            arrow = patches.FancyArrowPatch((2.5, y_positions[i]), (6.5, 5),\n",
        "                                          arrowstyle='->', mutation_scale=20,\n",
        "                                          linewidth=arrow_width*10, alpha=0.6,\n",
        "                                          color=source_color)\n",
        "            ax.add_patch(arrow)\n",
        "\n",
        "            # Percentage label\n",
        "            ax.text(4.5, y_positions[i]+0.2, f\"{item['value']:.1f}%\",\n",
        "                   ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        # Target box\n",
        "        target_rect = patches.Rectangle((7, 4.5), 2, 1,\n",
        "                                      facecolor=COLORS['final'], alpha=0.7, edgecolor='black')\n",
        "        ax.add_patch(target_rect)\n",
        "        target_text = 'Final Article' if mode == 'individual' else 'Final Articles'\n",
        "        ax.text(8, 5, target_text, ha='center', va='center', fontweight='bold')\n",
        "\n",
        "        ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
        "        ax.axis('off')\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def create_modification_intensity_chart(self, data, title=\"Content Modification Intensity\", mode=\"individual\"):\n",
        "        \"\"\"Create a chart showing modification intensity levels\"\"\"\n",
        "\n",
        "        if mode == \"individual\":\n",
        "            mod_dist = data['attribution_analysis']['statistics']['modification_distribution']\n",
        "            categories = list(mod_dist.keys())\n",
        "            values = [mod_dist[cat]['percentage'] for cat in categories]\n",
        "        else:\n",
        "            # Trend analysis\n",
        "            avg_data = data['attribution_trends']['averages']\n",
        "            categories = []\n",
        "            values = []\n",
        "            for key, value in avg_data.items():\n",
        "                if key.startswith('modification_'):\n",
        "                    cat = key.replace('modification_', '').replace('_', ' ').title()\n",
        "                    categories.append(cat)\n",
        "                    values.append(value)\n",
        "\n",
        "        # Create horizontal bar chart\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # Color mapping for modification levels\n",
        "        mod_colors = [COLORS.get(cat.lower().replace(' ', '_'), '#CCCCCC') for cat in categories]\n",
        "\n",
        "        bars = ax.barh(categories, values, color=mod_colors, alpha=0.8, edgecolor='black', linewidth=1)\n",
        "\n",
        "        # Add percentage labels on bars\n",
        "        for bar, value in zip(bars, values):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 1, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{value:.1f}%', ha='left', va='center', fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel('Percentage of Content', fontsize=12, fontweight='bold')\n",
        "        ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_xlim(0, max(values) * 1.2)\n",
        "\n",
        "        # Add grid for readability\n",
        "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "        ax.set_axisbelow(True)\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def create_word_count_progression(self, data, title=\"Word Count Progression\", mode=\"individual\"):\n",
        "        \"\"\"Create a line chart showing word count changes across versions\"\"\"\n",
        "\n",
        "        if mode == \"individual\":\n",
        "            processing = data['processing_summary']\n",
        "            versions = ['draft', 'refined', 'edited', 'final']\n",
        "            word_counts = []\n",
        "            version_labels = []\n",
        "\n",
        "            for version in versions:\n",
        "                if version in processing:\n",
        "                    word_counts.append(processing[version]['word_count'])\n",
        "                    version_labels.append(version.title())\n",
        "        else:\n",
        "            # Trend analysis - show averages\n",
        "            avg_data = data['word_count_trends']['averages']\n",
        "            versions = ['draft', 'refined', 'edited', 'final']\n",
        "            word_counts = []\n",
        "            version_labels = []\n",
        "\n",
        "            for version in versions:\n",
        "                if version in avg_data:\n",
        "                    word_counts.append(avg_data[version])\n",
        "                    version_labels.append(version.title())\n",
        "\n",
        "        # Create line chart\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        line = ax.plot(version_labels, word_counts, marker='o', linewidth=3,\n",
        "                      markersize=8, color=COLORS['draft'], markerfacecolor=COLORS['final'])\n",
        "\n",
        "        # Add value labels on points\n",
        "        for i, (label, count) in enumerate(zip(version_labels, word_counts)):\n",
        "            ax.annotate(f'{int(count)}', (i, count), textcoords=\"offset points\",\n",
        "                       xytext=(0,10), ha='center', fontweight='bold')\n",
        "\n",
        "        ax.set_ylabel('Word Count', fontsize=12, fontweight='bold')\n",
        "        ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.grid(True, alpha=0.3, linestyle='--')\n",
        "        ax.set_axisbelow(True)\n",
        "\n",
        "        # Calculate and show percentage change\n",
        "        if len(word_counts) >= 2:\n",
        "            change = ((word_counts[-1] - word_counts[0]) / word_counts[0]) * 100\n",
        "            change_text = f\"Overall change: {change:+.1f}%\"\n",
        "            ax.text(0.02, 0.98, change_text, transform=ax.transAxes,\n",
        "                   fontsize=11, fontweight='bold', verticalalignment='top',\n",
        "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def create_similarity_heatmap(self, data, title=\"Version Similarity Matrix\", mode=\"individual\"):\n",
        "        \"\"\"Create a heatmap showing similarities between versions\"\"\"\n",
        "\n",
        "        if mode == \"individual\":\n",
        "            similarity_data = data['similarity_analysis']['sequential_analysis']\n",
        "\n",
        "            # Build similarity matrix\n",
        "            versions = ['Draft', 'Refined', 'Edited', 'Final']\n",
        "            matrix = np.zeros((len(versions), len(versions)))\n",
        "            np.fill_diagonal(matrix, 1.0)  # Perfect similarity with self\n",
        "\n",
        "            # Fill in the sequential similarities\n",
        "            for seq in similarity_data:\n",
        "                pair = seq['version_pair']\n",
        "                similarity = seq['full_text']['combined']\n",
        "\n",
        "                # Parse version pair (e.g., \"draft_to_refined\")\n",
        "                from_version, to_version = pair.split('_to_')\n",
        "                from_idx = versions.index(from_version.title())\n",
        "                to_idx = versions.index(to_version.title())\n",
        "\n",
        "                matrix[from_idx][to_idx] = similarity\n",
        "                matrix[to_idx][from_idx] = similarity  # Make symmetric\n",
        "\n",
        "        else:\n",
        "            # For trend analysis, show average similarities\n",
        "            similarity_trends = data['similarity_trends']\n",
        "\n",
        "            # Create a simplified matrix for trends\n",
        "            versions = ['Draft', 'Final']\n",
        "            matrix = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
        "\n",
        "            if similarity_trends['averages'].get('draft_to_final'):\n",
        "                avg_sim = similarity_trends['averages']['draft_to_final']['combined']\n",
        "                matrix[0][1] = avg_sim\n",
        "                matrix[1][0] = avg_sim\n",
        "\n",
        "        # Create heatmap\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "        im = ax.imshow(matrix, cmap='RdYlGn', aspect='equal', vmin=0, vmax=1)\n",
        "\n",
        "        # Add text annotations\n",
        "        for i in range(len(versions)):\n",
        "            for j in range(len(versions)):\n",
        "                text = ax.text(j, i, f'{matrix[i, j]:.2f}',\n",
        "                             ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
        "\n",
        "        ax.set_xticks(range(len(versions)))\n",
        "        ax.set_yticks(range(len(versions)))\n",
        "        ax.set_xticklabels(versions)\n",
        "        ax.set_yticklabels(versions)\n",
        "        ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "        # Add colorbar\n",
        "        cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
        "        cbar.set_label('Similarity Score', rotation=270, labelpad=20, fontweight='bold')\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def create_summary_dashboard(self, data, title_prefix=\"Article Analysis\", mode=\"individual\"):\n",
        "        \"\"\"Create a comprehensive dashboard with multiple visualizations\"\"\"\n",
        "\n",
        "        # Create subplots\n",
        "        fig = plt.figure(figsize=(16, 12))\n",
        "\n",
        "        # Title\n",
        "        main_title = f\"{title_prefix} Dashboard\"\n",
        "        if mode == \"individual\":\n",
        "            main_title += f\" - {data['article_name']}\"\n",
        "\n",
        "        fig.suptitle(main_title, fontsize=18, fontweight='bold', y=0.95)\n",
        "\n",
        "        # Create individual charts and save them\n",
        "        charts = {}\n",
        "\n",
        "        # Content Flow Chart\n",
        "        charts['flow'] = self.create_content_flow_chart(data, \"Content Attribution\", mode)\n",
        "\n",
        "        # Modification Intensity\n",
        "        charts['modification'] = self.create_modification_intensity_chart(data, \"Modification Intensity\", mode)\n",
        "\n",
        "        # Word Count Progression\n",
        "        charts['word_count'] = self.create_word_count_progression(data, \"Word Count Evolution\", mode)\n",
        "\n",
        "        # Similarity Heatmap\n",
        "        charts['similarity'] = self.create_similarity_heatmap(data, \"Version Similarities\", mode)\n",
        "\n",
        "        return charts\n",
        "\n",
        "    def save_visualizations(self, charts, prefix=\"analysis\", formats=['png', 'svg']):\n",
        "        \"\"\"Save all visualizations in multiple formats\"\"\"\n",
        "\n",
        "        if not self.output_path:\n",
        "            print(\"‚ö†Ô∏è No output path specified - visualizations not saved\")\n",
        "            return {}\n",
        "\n",
        "        os.makedirs(self.output_path, exist_ok=True)\n",
        "        saved_files = {}\n",
        "\n",
        "        for chart_name, fig in charts.items():\n",
        "            for fmt in formats:\n",
        "                filename = f\"{prefix}_{chart_name}.{fmt}\"\n",
        "                filepath = os.path.join(self.output_path, filename)\n",
        "\n",
        "                fig.savefig(filepath, dpi=300, bbox_inches='tight',\n",
        "                           facecolor='white', edgecolor='none')\n",
        "\n",
        "                if chart_name not in saved_files:\n",
        "                    saved_files[chart_name] = []\n",
        "                saved_files[chart_name].append(filepath)\n",
        "\n",
        "        # Close figures to free memory\n",
        "        for fig in charts.values():\n",
        "            plt.close(fig)\n",
        "\n",
        "        print(f\"üíæ Visualizations saved to: {self.output_path}\")\n",
        "        for chart_name, files in saved_files.items():\n",
        "            print(f\"  üìä {chart_name}: {len(files)} formats\")\n",
        "\n",
        "        return saved_files\n",
        "\n",
        "print(\"üé® VisualizationEngine class loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZnFq9pfjhdL",
        "outputId": "0843a006-da69-4a5f-c7cd-873f4eb55c46"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Visualization dependencies loaded successfully!\n",
            "üé® Custom color palette configured\n",
            "üé® VisualizationEngine class loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 2: INTERACTIVE VISUALIZATION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def create_interactive_flow_chart(data, mode=\"individual\"):\n",
        "    \"\"\"Create an interactive Plotly flow/sankey diagram\"\"\"\n",
        "\n",
        "    if mode == \"individual\":\n",
        "        attribution_stats = data['attribution_analysis']['statistics']\n",
        "        origin_dist = attribution_stats['origin_distribution']\n",
        "\n",
        "        # Prepare data for Sankey diagram\n",
        "        sources = []\n",
        "        targets = []\n",
        "        values = []\n",
        "        labels = []\n",
        "\n",
        "        # Add all source versions\n",
        "        for version in ['draft', 'refined', 'edited']:\n",
        "            if version in origin_dist:\n",
        "                labels.append(version.title())\n",
        "\n",
        "        # Add new content and final\n",
        "        if 'new_in_final' in origin_dist:\n",
        "            labels.append('New Content')\n",
        "        labels.append('Final Article')\n",
        "\n",
        "        # Create connections\n",
        "        final_idx = len(labels) - 1\n",
        "\n",
        "        for version, stats in origin_dist.items():\n",
        "            if version != 'new_in_final':\n",
        "                source_idx = labels.index(version.title())\n",
        "                sources.append(source_idx)\n",
        "                targets.append(final_idx)\n",
        "                values.append(stats['percentage'])\n",
        "\n",
        "        # Add new content if exists\n",
        "        if 'new_in_final' in origin_dist:\n",
        "            new_idx = labels.index('New Content')\n",
        "            sources.append(new_idx)\n",
        "            targets.append(final_idx)\n",
        "            values.append(origin_dist['new_in_final']['percentage'])\n",
        "\n",
        "    else:\n",
        "        # Trend analysis mode\n",
        "        avg_data = data['attribution_trends']['averages']\n",
        "        labels = []\n",
        "        sources = []\n",
        "        targets = []\n",
        "        values = []\n",
        "\n",
        "        # Build labels and connections for trend data\n",
        "        for key, value in avg_data.items():\n",
        "            if key.startswith('origin_') and value > 0:\n",
        "                version = key.replace('origin_', '')\n",
        "                if version != 'new_in_final':\n",
        "                    labels.append(version.title())\n",
        "                else:\n",
        "                    labels.append('New Content')\n",
        "\n",
        "        labels.append('Final Articles')\n",
        "        final_idx = len(labels) - 1\n",
        "\n",
        "        # Create connections\n",
        "        i = 0\n",
        "        for key, value in avg_data.items():\n",
        "            if key.startswith('origin_') and value > 0:\n",
        "                sources.append(i)\n",
        "                targets.append(final_idx)\n",
        "                values.append(value)\n",
        "                i += 1\n",
        "\n",
        "    # Create Sankey diagram\n",
        "    fig = go.Figure(data=[go.Sankey(\n",
        "        node=dict(\n",
        "            pad=15,\n",
        "            thickness=20,\n",
        "            line=dict(color=\"black\", width=0.5),\n",
        "            label=labels,\n",
        "            color=[COLORS.get(label.lower().replace(' ', '_'), '#CCCCCC') for label in labels]\n",
        "        ),\n",
        "        link=dict(\n",
        "            source=sources,\n",
        "            target=targets,\n",
        "            value=values,\n",
        "            color=['rgba(255,107,107,0.4)' if i < len(sources) else 'rgba(150,206,180,0.4)'\n",
        "                   for i in range(len(sources))]\n",
        "        )\n",
        "    )])\n",
        "\n",
        "    title = \"Content Flow Analysis\"\n",
        "    if mode == \"individual\":\n",
        "        title += f\" - {data['article_name']}\"\n",
        "\n",
        "    fig.update_layout(\n",
        "        title_text=title,\n",
        "        font_size=12,\n",
        "        height=600\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "def create_interactive_trends_chart(trend_data):\n",
        "    \"\"\"Create interactive trend charts for multiple articles over time\"\"\"\n",
        "\n",
        "    # Extract data for trends over time\n",
        "    attribution_by_article = trend_data['attribution_trends']['by_article']\n",
        "\n",
        "    if not attribution_by_article:\n",
        "        print(\"No trend data available\")\n",
        "        return None\n",
        "\n",
        "    # Prepare data for plotting\n",
        "    dates = []\n",
        "    draft_retention = []\n",
        "    edited_dominance = []\n",
        "    new_content = []\n",
        "    article_names = []\n",
        "\n",
        "    for article in attribution_by_article:\n",
        "        dates.append(article['publication_date'])\n",
        "        article_names.append(article['article_name'])\n",
        "\n",
        "        # Extract percentages\n",
        "        draft_retention.append(article['origin_percentages'].get('draft', 0))\n",
        "        edited_dominance.append(article['origin_percentages'].get('edited', 0))\n",
        "        new_content.append(article['origin_percentages'].get('new_in_final', 0))\n",
        "\n",
        "    # Create multi-line chart\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add traces for each metric\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=dates, y=draft_retention,\n",
        "        mode='lines+markers',\n",
        "        name='Draft Retention %',\n",
        "        line=dict(color=COLORS['draft'], width=3),\n",
        "        marker=dict(size=8),\n",
        "        hovertemplate='<b>%{text}</b><br>Date: %{x}<br>Draft Retention: %{y:.1f}%<extra></extra>',\n",
        "        text=article_names\n",
        "    ))\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=dates, y=edited_dominance,\n",
        "        mode='lines+markers',\n",
        "        name='Edited Content %',\n",
        "        line=dict(color=COLORS['edited'], width=3),\n",
        "        marker=dict(size=8),\n",
        "        hovertemplate='<b>%{text}</b><br>Date: %{x}<br>Edited Content: %{y:.1f}%<extra></extra>',\n",
        "        text=article_names\n",
        "    ))\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=dates, y=new_content,\n",
        "        mode='lines+markers',\n",
        "        name='New Content %',\n",
        "        line=dict(color=COLORS['new_content'], width=3),\n",
        "        marker=dict(size=8),\n",
        "        hovertemplate='<b>%{text}</b><br>Date: %{x}<br>New Content: %{y:.1f}%<extra></extra>',\n",
        "        text=article_names\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title='Content Attribution Trends Over Time',\n",
        "        xaxis_title='Publication Date',\n",
        "        yaxis_title='Percentage of Final Content',\n",
        "        hovermode='x unified',\n",
        "        height=600,\n",
        "        showlegend=True\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "print(\"üöÄ Interactive visualization functions loaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlHV6DxKjrLW",
        "outputId": "64e44b59-ff9d-472b-b329-fedfc2aba091"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Interactive visualization functions loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 3: EXECUTION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def visualize_individual_article(analysis_data, output_path=None, save_files=True):\n",
        "    \"\"\"Create visualizations for a single article analysis\"\"\"\n",
        "\n",
        "    print(f\"üé® Creating visualizations for: {analysis_data['article_name']}\")\n",
        "\n",
        "    # Initialize visualization engine\n",
        "    viz_engine = VisualizationEngine(output_path)\n",
        "\n",
        "    # Create static charts\n",
        "    print(\"üìä Generating static visualizations...\")\n",
        "    charts = viz_engine.create_summary_dashboard(analysis_data, mode=\"individual\")\n",
        "\n",
        "    # Create interactive chart\n",
        "    print(\"üîß Creating interactive flow chart...\")\n",
        "    interactive_flow = create_interactive_flow_chart(analysis_data, mode=\"individual\")\n",
        "\n",
        "    # Save files if requested\n",
        "    saved_files = {}\n",
        "    if save_files and output_path:\n",
        "        saved_files = viz_engine.save_visualizations(\n",
        "            charts,\n",
        "            prefix=f\"{analysis_data['article_name']}_analysis\",\n",
        "            formats=['png', 'svg']\n",
        "        )\n",
        "\n",
        "        # Save interactive chart\n",
        "        interactive_file = os.path.join(output_path, f\"{analysis_data['article_name']}_interactive_flow.html\")\n",
        "        interactive_flow.write_html(interactive_file)\n",
        "        saved_files['interactive_flow'] = [interactive_file]\n",
        "        print(f\"üíæ Interactive chart saved: {interactive_file}\")\n",
        "\n",
        "    # Display interactive chart\n",
        "    interactive_flow.show()\n",
        "\n",
        "    return charts, interactive_flow, saved_files\n",
        "\n",
        "def visualize_trend_analysis(trend_data, output_path=None, save_files=True):\n",
        "    \"\"\"Create visualizations for trend analysis\"\"\"\n",
        "\n",
        "    period = trend_data['period']\n",
        "    print(f\"üìà Creating trend visualizations for period: {period}\")\n",
        "\n",
        "    # Initialize visualization engine\n",
        "    viz_engine = VisualizationEngine(output_path)\n",
        "\n",
        "    # Create static charts\n",
        "    print(\"üìä Generating static trend visualizations...\")\n",
        "    charts = viz_engine.create_summary_dashboard(trend_data, f\"Trend Analysis - {period}\", mode=\"trend\")\n",
        "\n",
        "    # Create interactive charts\n",
        "    print(\"üîß Creating interactive trend charts...\")\n",
        "    interactive_flow = create_interactive_flow_chart(trend_data, mode=\"trend\")\n",
        "    interactive_trends = create_interactive_trends_chart(trend_data)\n",
        "\n",
        "    # Save files if requested\n",
        "    saved_files = {}\n",
        "    if save_files and output_path:\n",
        "        saved_files = viz_engine.save_visualizations(\n",
        "            charts,\n",
        "            prefix=f\"trend_analysis_{period}\",\n",
        "            formats=['png', 'svg']\n",
        "        )\n",
        "\n",
        "        # Save interactive charts\n",
        "        if output_path:\n",
        "            flow_file = os.path.join(output_path, f\"trend_flow_{period}.html\")\n",
        "            interactive_flow.write_html(flow_file)\n",
        "            saved_files['interactive_flow'] = [flow_file]\n",
        "\n",
        "            if interactive_trends:\n",
        "                trends_file = os.path.join(output_path, f\"trend_timeline_{period}.html\")\n",
        "                interactive_trends.write_html(trends_file)\n",
        "                saved_files['interactive_trends'] = [trends_file]\n",
        "                print(f\"üíæ Interactive charts saved to: {output_path}\")\n",
        "\n",
        "    # Display interactive charts\n",
        "    interactive_flow.show()\n",
        "    if interactive_trends:\n",
        "        interactive_trends.show()\n",
        "\n",
        "    return charts, {'flow': interactive_flow, 'trends': interactive_trends}, saved_files\n",
        "\n",
        "def quick_visualize(data, data_type=\"individual\", output_path=None):\n",
        "    \"\"\"Quick visualization function for any analysis data\"\"\"\n",
        "\n",
        "    if data_type == \"individual\":\n",
        "        return visualize_individual_article(data, output_path, save_files=bool(output_path))\n",
        "    elif data_type == \"trend\":\n",
        "        return visualize_trend_analysis(data, output_path, save_files=bool(output_path))\n",
        "    else:\n",
        "        print(f\"‚ùå Unknown data type: {data_type}\")\n",
        "        return None\n",
        "\n",
        "print(\"üéØ Execution functions loaded!\")\n",
        "print(\"\\nUsage examples:\")\n",
        "print(\"# Visualize individual article (using existing analysis data)\")\n",
        "print(\"charts, interactive, files = visualize_individual_article(combined_results, '/your/output/path')\")\n",
        "print(\"\\n# Visualize trend analysis\")\n",
        "print(\"charts, interactive, files = visualize_trend_analysis(trend_results, '/your/output/path')\")\n",
        "print(\"\\n# Quick visualization without saving\")\n",
        "print(\"quick_visualize(combined_results, 'individual')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6l2aIb8j5DK",
        "outputId": "8783f188-7610-4c12-99b6-25d5786df51d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Execution functions loaded!\n",
            "\n",
            "Usage examples:\n",
            "# Visualize individual article (using existing analysis data)\n",
            "charts, interactive, files = visualize_individual_article(combined_results, '/your/output/path')\n",
            "\n",
            "# Visualize trend analysis\n",
            "charts, interactive, files = visualize_trend_analysis(trend_results, '/your/output/path')\n",
            "\n",
            "# Quick visualization without saving\n",
            "quick_visualize(combined_results, 'individual')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 4: FOOTER METRICS GENERATOR\n",
        "# =============================================================================\n",
        "\n",
        "def generate_article_footer_graphics(analysis_data, output_path=None):\n",
        "    \"\"\"Generate clean, publication-ready graphics for article footers\"\"\"\n",
        "\n",
        "    print(f\"üìÑ Generating footer graphics for: {analysis_data['article_name']}\")\n",
        "\n",
        "    # Create a clean, minimal style for footer graphics\n",
        "    plt.style.use('default')\n",
        "\n",
        "    # Footer-specific color palette (more muted)\n",
        "    footer_colors = {\n",
        "        'primary': '#2E86AB',\n",
        "        'secondary': '#A23B72',\n",
        "        'accent': '#F18F01',\n",
        "        'neutral': '#C73E1D'\n",
        "    }\n",
        "\n",
        "    # Create a compact summary visualization\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
        "    fig.suptitle(f'Content Analysis Summary - {analysis_data[\"article_name\"]}',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "    # 1. Content Origins (Pie Chart)\n",
        "    attribution_stats = analysis_data['attribution_analysis']['statistics']\n",
        "    origin_dist = attribution_stats['origin_distribution']\n",
        "\n",
        "    labels = []\n",
        "    sizes = []\n",
        "    colors = []\n",
        "\n",
        "    for version, stats in origin_dist.items():\n",
        "        if version != 'new_in_final':\n",
        "            labels.append(f\"{version.title()}\\n{stats['percentage']:.1f}%\")\n",
        "            sizes.append(stats['percentage'])\n",
        "            colors.append(footer_colors['primary'])\n",
        "\n",
        "    if 'new_in_final' in origin_dist:\n",
        "        labels.append(f\"New Content\\n{origin_dist['new_in_final']['percentage']:.1f}%\")\n",
        "        sizes.append(origin_dist['new_in_final']['percentage'])\n",
        "        colors.append(footer_colors['accent'])\n",
        "\n",
        "    ax1.pie(sizes, labels=labels, colors=colors, autopct='', startangle=90)\n",
        "    ax1.set_title('Content Origins', fontweight='bold')\n",
        "\n",
        "    # 2. Word Count Progression (Bar Chart)\n",
        "    processing = analysis_data['processing_summary']\n",
        "    versions = ['Draft', 'Refined', 'Edited', 'Final']\n",
        "    word_counts = []\n",
        "\n",
        "    for version in ['draft', 'refined', 'edited', 'final']:\n",
        "        if version in processing:\n",
        "            word_counts.append(processing[version]['word_count'])\n",
        "        else:\n",
        "            word_counts.append(0)\n",
        "\n",
        "    bars = ax2.bar(versions, word_counts, color=footer_colors['secondary'], alpha=0.7)\n",
        "    ax2.set_title('Word Count Evolution', fontweight='bold')\n",
        "    ax2.set_ylabel('Words')\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, count in zip(bars, word_counts):\n",
        "        if count > 0:\n",
        "            height = bar.get_height()\n",
        "            ax2.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
        "                    f'{int(count)}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # 3. Modification Intensity (Horizontal Bar)\n",
        "    mod_dist = attribution_stats['modification_distribution']\n",
        "    mod_categories = []\n",
        "    mod_values = []\n",
        "\n",
        "    for category, stats in mod_dist.items():\n",
        "        clean_cat = category.replace('_', ' ').title()\n",
        "        mod_categories.append(clean_cat)\n",
        "        mod_values.append(stats['percentage'])\n",
        "\n",
        "    bars = ax3.barh(mod_categories, mod_values, color=footer_colors['neutral'], alpha=0.7)\n",
        "    ax3.set_title('Modification Levels', fontweight='bold')\n",
        "    ax3.set_xlabel('Percentage')\n",
        "\n",
        "    # Add percentage labels\n",
        "    for bar, value in zip(bars, mod_values):\n",
        "        width = bar.get_width()\n",
        "        ax3.text(width + 1, bar.get_y() + bar.get_height()/2.,\n",
        "                f'{value:.1f}%', ha='left', va='center', fontsize=9)\n",
        "\n",
        "    # 4. Key Metrics Summary (Text)\n",
        "    ax4.axis('off')\n",
        "\n",
        "    # Calculate key metrics\n",
        "    draft_to_final = analysis_data['similarity_analysis']['draft_to_final']\n",
        "    similarity_pct = draft_to_final['full_text']['combined'] * 100 if draft_to_final else 0\n",
        "\n",
        "    word_change = 0\n",
        "    if 'draft' in processing and 'final' in processing:\n",
        "        draft_words = processing['draft']['word_count']\n",
        "        final_words = processing['final']['word_count']\n",
        "        word_change = ((final_words - draft_words) / draft_words) * 100\n",
        "\n",
        "    # Display key metrics as text\n",
        "    metrics_text = f\"\"\"Key Metrics:\n",
        "\n",
        "Draft-Final Similarity: {similarity_pct:.1f}%\n",
        "\n",
        "Word Count Change: {word_change:+.1f}%\n",
        "\n",
        "Total Sentences: {attribution_stats['total_sentences']}\n",
        "\n",
        "Analysis Date: {datetime.now().strftime('%Y-%m-%d')}\"\"\"\n",
        "\n",
        "    ax4.text(0.1, 0.9, metrics_text, transform=ax4.transAxes,\n",
        "             fontsize=11, verticalalignment='top', fontweight='bold',\n",
        "             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.3))\n",
        "\n",
        "    ax4.set_title('Summary Statistics', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save footer graphic\n",
        "    footer_files = {}\n",
        "    if output_path:\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        # Save in multiple formats for footer use\n",
        "        for fmt in ['png', 'svg', 'pdf']:\n",
        "            footer_file = os.path.join(output_path, f\"{analysis_data['article_name']}_footer.{fmt}\")\n",
        "            fig.savefig(footer_file, dpi=300, bbox_inches='tight',\n",
        "                       facecolor='white', edgecolor='none')\n",
        "            footer_files[fmt] = footer_file\n",
        "\n",
        "        print(f\"üìÑ Footer graphics saved: {len(footer_files)} formats\")\n",
        "\n",
        "    return fig, footer_files\n",
        "\n",
        "def create_minimal_attribution_chart(analysis_data, output_path=None):\n",
        "    \"\"\"Create a minimal, clean chart suitable for article footers\"\"\"\n",
        "\n",
        "    # Very simple pie chart for content attribution\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "\n",
        "    attribution_stats = analysis_data['attribution_analysis']['statistics']\n",
        "    origin_dist = attribution_stats['origin_distribution']\n",
        "\n",
        "    # Aggregate into simple categories\n",
        "    ai_generated = 0  # draft + refined\n",
        "    human_edited = 0  # edited\n",
        "    new_content = 0   # new_in_final\n",
        "\n",
        "    for version, stats in origin_dist.items():\n",
        "        if version in ['draft', 'refined']:\n",
        "            ai_generated += stats['percentage']\n",
        "        elif version == 'edited':\n",
        "            human_edited = stats['percentage']\n",
        "        elif version == 'new_in_final':\n",
        "            new_content = stats['percentage']\n",
        "\n",
        "    # Create simple pie chart\n",
        "    labels = []\n",
        "    sizes = []\n",
        "    colors = ['#FF6B6B', '#4ECDC4', '#FECA57']\n",
        "\n",
        "    if ai_generated > 0:\n",
        "        labels.append(f'AI Generated\\n{ai_generated:.1f}%')\n",
        "        sizes.append(ai_generated)\n",
        "\n",
        "    if human_edited > 0:\n",
        "        labels.append(f'Human Edited\\n{human_edited:.1f}%')\n",
        "        sizes.append(human_edited)\n",
        "\n",
        "    if new_content > 0:\n",
        "        labels.append(f'New Content\\n{new_content:.1f}%')\n",
        "        sizes.append(new_content)\n",
        "\n",
        "    pie_result = ax.pie(sizes, labels=labels, colors=colors[:len(sizes)],\n",
        "                       autopct='', startangle=90, textprops={'fontsize': 10})\n",
        "\n",
        "    # ax.pie returns different number of values based on parameters\n",
        "    # We only need the wedges for our purposes\n",
        "    wedges = pie_result[0] if isinstance(pie_result, tuple) else pie_result\n",
        "\n",
        "    ax.set_title(f'Content Attribution - {analysis_data[\"article_name\"]}',\n",
        "                fontsize=12, fontweight='bold', pad=10)\n",
        "\n",
        "    # Save minimal chart\n",
        "    minimal_files = {}\n",
        "    if output_path:\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        for fmt in ['png', 'svg']:\n",
        "            minimal_file = os.path.join(output_path, f\"{analysis_data['article_name']}_minimal.{fmt}\")\n",
        "            fig.savefig(minimal_file, dpi=300, bbox_inches='tight',\n",
        "                       facecolor='white', edgecolor='none')\n",
        "            minimal_files[fmt] = minimal_file\n",
        "\n",
        "        print(f\"üìä Minimal chart saved: {len(minimal_files)} formats\")\n",
        "\n",
        "    return fig, minimal_files\n",
        "\n",
        "print(\"üìÑ Footer graphics functions loaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv070Iw0j-wV",
        "outputId": "db063987-779a-476d-c6ee-46a37c2bad91"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Footer graphics functions loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 5: COMPLETE VISUALIZATION WORKFLOW\n",
        "# =============================================================================\n",
        "\n",
        "def create_complete_visualization_suite(analysis_data, output_path=None, data_type=\"individual\"):\n",
        "    \"\"\"Create a complete suite of visualizations for any analysis data\"\"\"\n",
        "\n",
        "    print(f\"üé® Creating complete visualization suite...\")\n",
        "    print(f\"üìä Data type: {data_type}\")\n",
        "\n",
        "    if not output_path:\n",
        "        print(\"‚ö†Ô∏è No output path provided - visualizations will not be saved\")\n",
        "\n",
        "    all_outputs = {\n",
        "        'static_charts': {},\n",
        "        'interactive_charts': {},\n",
        "        'footer_graphics': {},\n",
        "        'saved_files': {}\n",
        "    }\n",
        "\n",
        "    # Create main visualizations\n",
        "    if data_type == \"individual\":\n",
        "        # Individual article visualizations\n",
        "        charts, interactive, files = visualize_individual_article(\n",
        "            analysis_data, output_path, save_files=bool(output_path)\n",
        "        )\n",
        "\n",
        "        all_outputs['static_charts'] = charts\n",
        "        all_outputs['interactive_charts'] = {'flow': interactive}\n",
        "        all_outputs['saved_files'].update(files)\n",
        "\n",
        "        # Create footer graphics\n",
        "        if output_path:\n",
        "            footer_fig, footer_files = generate_article_footer_graphics(analysis_data, output_path)\n",
        "            minimal_fig, minimal_files = create_minimal_attribution_chart(analysis_data, output_path)\n",
        "\n",
        "            all_outputs['footer_graphics']['full'] = footer_fig\n",
        "            all_outputs['footer_graphics']['minimal'] = minimal_fig\n",
        "            all_outputs['saved_files']['footer'] = footer_files\n",
        "            all_outputs['saved_files']['minimal'] = minimal_files\n",
        "\n",
        "            # Close footer figures\n",
        "            plt.close(footer_fig)\n",
        "            plt.close(minimal_fig)\n",
        "\n",
        "    elif data_type == \"trend\":\n",
        "        # Trend analysis visualizations\n",
        "        charts, interactive, files = visualize_trend_analysis(\n",
        "            analysis_data, output_path, save_files=bool(output_path)\n",
        "        )\n",
        "\n",
        "        all_outputs['static_charts'] = charts\n",
        "        all_outputs['interactive_charts'] = interactive\n",
        "        all_outputs['saved_files'].update(files)\n",
        "\n",
        "    print(f\"‚úÖ Visualization suite complete!\")\n",
        "    if output_path:\n",
        "        print(f\"üìÅ All files saved to: {output_path}\")\n",
        "\n",
        "    return all_outputs\n",
        "\n",
        "def visualize_from_existing_data(analysis_data_or_file, output_path=None, data_type=\"auto\"):\n",
        "    \"\"\"Load and visualize from existing analysis data or file\"\"\"\n",
        "\n",
        "    # Handle file input\n",
        "    if isinstance(analysis_data_or_file, str):\n",
        "        print(f\"üìñ Loading analysis data from: {analysis_data_or_file}\")\n",
        "        with open(analysis_data_or_file, 'r', encoding='utf-8') as f:\n",
        "            analysis_data = json.load(f)\n",
        "    else:\n",
        "        analysis_data = analysis_data_or_file\n",
        "\n",
        "    # Auto-detect data type if not specified\n",
        "    if data_type == \"auto\":\n",
        "        if 'article_name' in analysis_data and 'attribution_analysis' in analysis_data:\n",
        "            data_type = \"individual\"\n",
        "        elif 'period' in analysis_data and 'attribution_trends' in analysis_data:\n",
        "            data_type = \"trend\"\n",
        "        else:\n",
        "            print(\"‚ùå Could not auto-detect data type\")\n",
        "            return None\n",
        "\n",
        "    print(f\"üîç Detected data type: {data_type}\")\n",
        "\n",
        "    # Create visualizations\n",
        "    return create_complete_visualization_suite(analysis_data, output_path, data_type)\n",
        "\n",
        "# Quick access functions\n",
        "def viz_article(analysis_data, output_path=None):\n",
        "    \"\"\"Quick function to visualize individual article\"\"\"\n",
        "    # Use the same output path structure as the analysis if not provided\n",
        "    if output_path is None and 'article_metadata' in analysis_data:\n",
        "        # Try to derive output path from the analysis data\n",
        "        input_path = analysis_data['article_metadata'].get('input_path')\n",
        "        if input_path:\n",
        "            output_path = os.path.join(input_path, 'output')\n",
        "\n",
        "    return create_complete_visualization_suite(analysis_data, output_path, \"individual\")\n",
        "\n",
        "def viz_trends(trend_data, output_path=None):\n",
        "    \"\"\"Quick function to visualize trend analysis\"\"\"\n",
        "    return create_complete_visualization_suite(trend_data, output_path, \"trend\")\n",
        "\n",
        "print(\"üöÄ Complete visualization workflow loaded!\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä VISUALIZATION SYSTEM READY!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nQuick usage:\")\n",
        "print(\"# Visualize your existing analysis data\")\n",
        "print(\"viz_outputs = viz_article(combined_results, '/your/output/path')\")\n",
        "print(\"\\n# Visualize trend analysis\")\n",
        "print(\"trend_viz = viz_trends(trend_results, '/your/output/path')\")\n",
        "print(\"\\n# Create footer graphics only\")\n",
        "print(\"footer_fig, files = generate_article_footer_graphics(combined_results, '/path')\")\n",
        "print(\"\\nAll visualizations include:\")\n",
        "print(\"  üìà Static charts (PNG, SVG)\")\n",
        "print(\"  üîß Interactive charts (HTML)\")\n",
        "print(\"  üìÑ Footer graphics (publication-ready)\")\n",
        "print(\"  üíæ Multiple file formats\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW4CPVrvkD0-",
        "outputId": "dc622b18-3cdc-4356-ba96-18c68e17da9e"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Complete visualization workflow loaded!\n",
            "\n",
            "============================================================\n",
            "üìä VISUALIZATION SYSTEM READY!\n",
            "============================================================\n",
            "\n",
            "Quick usage:\n",
            "# Visualize your existing analysis data\n",
            "viz_outputs = viz_article(combined_results, '/your/output/path')\n",
            "\n",
            "# Visualize trend analysis\n",
            "trend_viz = viz_trends(trend_results, '/your/output/path')\n",
            "\n",
            "# Create footer graphics only\n",
            "footer_fig, files = generate_article_footer_graphics(combined_results, '/path')\n",
            "\n",
            "All visualizations include:\n",
            "  üìà Static charts (PNG, SVG)\n",
            "  üîß Interactive charts (HTML)\n",
            "  üìÑ Footer graphics (publication-ready)\n",
            "  üíæ Multiple file formats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "viz_outputs = viz_article(combined_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "TdR7tDxWkSJk",
        "outputId": "875ca328-6645-4f27-e9ed-665e83c24b99"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé® Creating complete visualization suite...\n",
            "üìä Data type: individual\n",
            "üé® Creating visualizations for: markup-languages\n",
            "üìä Generating static visualizations...\n",
            "üîß Creating interactive flow chart...\n",
            "üíæ Visualizations saved to: /content/drive/MyDrive/Google Drive/syntaxandempathy/30-articles/ai-vs-human/output\n",
            "  üìä flow: 2 formats\n",
            "  üìä modification: 2 formats\n",
            "  üìä word_count: 2 formats\n",
            "  üìä similarity: 2 formats\n",
            "üíæ Interactive chart saved: /content/drive/MyDrive/Google Drive/syntaxandempathy/30-articles/ai-vs-human/output/markup-languages_interactive_flow.html\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"7750f5f4-027d-4d60-9f6c-4199a27f6543\" class=\"plotly-graph-div\" style=\"height:600px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7750f5f4-027d-4d60-9f6c-4199a27f6543\")) {                    Plotly.newPlot(                        \"7750f5f4-027d-4d60-9f6c-4199a27f6543\",                        [{\"link\":{\"color\":[\"rgba(255,107,107,0.4)\",\"rgba(255,107,107,0.4)\",\"rgba(255,107,107,0.4)\"],\"source\":[1,0,2],\"target\":[3,3,3],\"value\":[85.71428571428571,2.380952380952381,11.904761904761903]},\"node\":{\"color\":[\"#FF6B6B\",\"#45B7D1\",\"#FECA57\",\"#CCCCCC\"],\"label\":[\"Draft\",\"Edited\",\"New Content\",\"Final Article\"],\"line\":{\"color\":\"black\",\"width\":0.5},\"pad\":15,\"thickness\":20},\"type\":\"sankey\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"bgcolor\":\"white\",\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"white\",\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"white\",\"subunitcolor\":\"#C8D4E3\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Content Flow Analysis - markup-languages\"},\"font\":{\"size\":12},\"height\":600},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7750f5f4-027d-4d60-9f6c-4199a27f6543');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Generating footer graphics for: markup-languages\n",
            "üìÑ Footer graphics saved: 3 formats\n",
            "üìä Minimal chart saved: 2 formats\n",
            "‚úÖ Visualization suite complete!\n",
            "üìÅ All files saved to: /content/drive/MyDrive/Google Drive/syntaxandempathy/30-articles/ai-vs-human/output\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}